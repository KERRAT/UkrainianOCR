{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset(datasetPath):\n",
    "\n",
    "  # List for storing data\n",
    "  data = []\n",
    "  \n",
    "  # List for storing labels\n",
    "  labels = []\n",
    "  \n",
    "  for row in open(datasetPath): #Openfile and start reading each row\n",
    "    #Split the row at every comma\n",
    "    row = row.split(\",\")\n",
    "    \n",
    "    #row[0] contains label\n",
    "    label = int(row[0])\n",
    "    \n",
    "    #Other all collumns contains pixel values make a saperate array for that\n",
    "    image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "    \n",
    "    #Reshaping image to 28 x 28 pixels\n",
    "    image = image.reshape((32, 32))\n",
    "    \n",
    "    #append image to data\n",
    "    data.append(image)\n",
    "    \n",
    "    #append label to labels\n",
    "    labels.append(label)\n",
    "    \n",
    "  #Converting data to numpy array of type float32\n",
    "  data = np.array(data, dtype='float32')\n",
    "  \n",
    "  #Converting labels to type int\n",
    "  labels = np.array(labels, dtype=\"int\")\n",
    "  \n",
    "  return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Data, Labels) = load_dataset(\"../data/uaset_without_columns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Data, Labels) = load_dataset(\"../data/uaset_extended_with_scale_factor_and_3x3_blure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# However, the architecture we're using is designed for 32x32 images,\n",
    "# So we need to resize them to 32x32\n",
    "\n",
    "Data = [cv2.resize(image, (32, 32)) for image in Data]\n",
    "Data = np.array(Data, dtype=\"float32\")\n",
    "\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "\n",
    "Data = np.expand_dims(Data, axis=-1)\n",
    "Data /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "le = LabelBinarizer()\n",
    "Labels = le.fit_transform(Labels)\n",
    "\n",
    "counts = Labels.sum(axis=0)\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals = Labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "  classWeight[i] = classTotals.max() / classTotals[i]\n",
    "  \n",
    "(trainX, testX, trainY, testY) = train_test_split(Data,\n",
    "\tLabels, test_size=0.30, stratify=Labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "rotation_range=6,\n",
    "zoom_range=0.20,\n",
    "width_shift_range=0.10,\n",
    "height_shift_range=0.10,\n",
    "shear_range=0.15,\n",
    "horizontal_flip=False,\n",
    "fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and apply BN\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\n",
    "\t\t# check if we are utilizing the CIFAR dataset\n",
    "\t\tif dataset == \"cifar\":\n",
    "\t\t\t# apply a single CONV layer\n",
    "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
    "\t\telif dataset == \"tiny_imagenet\":\n",
    "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\t\tmomentum=bnMom)(x)\n",
    "\t\t\tx = Activation(\"relu\")(x)\n",
    "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
    "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\t\t# loop over the number of stages\n",
    "\t\tfor i in range(0, len(stages)):\n",
    "\t\t\t# initialize the stride, then apply a residual module\n",
    "\t\t\t# used to reduce the spatial size of the input volume\n",
    "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t\t# loop over the number of layers in the stage\n",
    "\t\t\tfor j in range(0, stages[i] - 1):\n",
    "\t\t\t\t# apply a ResNet module\n",
    "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "INIT_LR = 1e-1\n",
    "BS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 1)    4           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 64)   576         batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 64)   256         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 32, 32, 64)   0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 32, 32, 16)   1024        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 16)   64          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 32, 32, 16)   0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 16)   2304        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 32, 32, 16)   64          conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 32, 32, 16)   0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 32, 32, 64)   1024        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 32, 32, 64)   4096        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 32, 32, 64)   0           conv2d_65[0][0]                  \n",
      "                                                                 conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 32, 32, 64)   256         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 32, 32, 64)   0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 32, 32, 16)   1024        activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 32, 32, 16)   64          conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 32, 32, 16)   0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 32, 32, 16)   2304        activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 32, 32, 16)   64          conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 32, 32, 16)   0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 32, 32, 64)   1024        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 32, 32, 64)   0           conv2d_69[0][0]                  \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 32, 32, 64)   256         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 32, 32, 64)   0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 32, 32, 16)   1024        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 32, 32, 16)   64          conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 32, 32, 16)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 32, 32, 16)   2304        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 32, 32, 16)   64          conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 32, 32, 16)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 32, 32, 64)   1024        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 32, 32, 64)   0           conv2d_72[0][0]                  \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 32, 32, 64)   256         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 32, 32, 64)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 32, 32, 32)   2048        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 32, 32, 32)   128         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 32, 32, 32)   0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 16, 16, 32)   9216        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 16, 16, 32)   128         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 16, 16, 32)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 16, 16, 128)  4096        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 16, 16, 128)  8192        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 16, 16, 128)  0           conv2d_75[0][0]                  \n",
      "                                                                 conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 16, 16, 128)  512         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 16, 16, 128)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 16, 16, 32)   4096        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 16, 16, 32)   128         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 16, 16, 32)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 16, 16, 32)   9216        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 16, 16, 32)   128         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 16, 16, 32)   0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 16, 16, 128)  4096        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 16, 16, 128)  0           conv2d_79[0][0]                  \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 16, 16, 128)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 16, 16, 32)   4096        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 16, 16, 32)   128         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 16, 16, 32)   0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 16, 16, 32)   9216        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 16, 16, 32)   128         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 16, 16, 32)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 16, 16, 128)  4096        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 16, 16, 128)  0           conv2d_82[0][0]                  \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 16, 16, 128)  512         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 16, 16, 128)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 16, 16, 64)   8192        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 16, 16, 64)   256         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 16, 16, 64)   0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 64)     36864       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 64)     256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 64)     0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 256)    16384       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 256)    32768       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_85[0][0]                  \n",
      "                                                                 conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 256)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 64)     16384       activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 64)     256         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 64)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 64)     36864       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 64)     256         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 64)     0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 256)    16384       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 8, 8, 256)    0           conv2d_89[0][0]                  \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 256)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 64)     16384       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 64)     256         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 64)     0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 64)     36864       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 64)     256         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 64)     0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 256)    16384       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 8, 8, 256)    0           conv2d_92[0][0]                  \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 256)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 1, 1, 256)    0           activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 90)           23130       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 90)           0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 341,022\n",
      "Trainable params: 336,860\n",
      "Non-trainable params: 4,162\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Додайте колбеки\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=12, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "\n",
    "callbacks_list = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 2.2951 - accuracy: 0.4942 - val_loss: 0.8827 - val_accuracy: 0.8238 - lr: 0.1000\n",
      "Epoch 2/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.8243 - accuracy: 0.8214 - val_loss: 0.8004 - val_accuracy: 0.8159 - lr: 0.1000\n",
      "Epoch 3/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.6948 - accuracy: 0.8551 - val_loss: 0.6024 - val_accuracy: 0.8872 - lr: 0.1000\n",
      "Epoch 4/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.6380 - accuracy: 0.8662 - val_loss: 0.5472 - val_accuracy: 0.8988 - lr: 0.1000\n",
      "Epoch 5/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.6003 - accuracy: 0.8757 - val_loss: 0.5441 - val_accuracy: 0.8924 - lr: 0.1000\n",
      "Epoch 6/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.5764 - accuracy: 0.8810 - val_loss: 0.5783 - val_accuracy: 0.8744 - lr: 0.1000\n",
      "Epoch 7/400\n",
      "1506/1506 [==============================] - 132s 87ms/step - loss: 0.5557 - accuracy: 0.8861 - val_loss: 0.5069 - val_accuracy: 0.9019 - lr: 0.1000\n",
      "Epoch 8/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.5418 - accuracy: 0.8881 - val_loss: 0.4965 - val_accuracy: 0.9031 - lr: 0.1000\n",
      "Epoch 9/400\n",
      "1506/1506 [==============================] - 133s 88ms/step - loss: 0.5273 - accuracy: 0.8913 - val_loss: 0.4940 - val_accuracy: 0.8970 - lr: 0.1000\n",
      "Epoch 10/400\n",
      "1506/1506 [==============================] - 134s 89ms/step - loss: 0.5166 - accuracy: 0.8935 - val_loss: 0.4679 - val_accuracy: 0.9071 - lr: 0.1000\n",
      "Epoch 11/400\n",
      "1506/1506 [==============================] - 137s 91ms/step - loss: 0.5060 - accuracy: 0.8953 - val_loss: 0.4858 - val_accuracy: 0.8963 - lr: 0.1000\n",
      "Epoch 12/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.4977 - accuracy: 0.8962 - val_loss: 0.4679 - val_accuracy: 0.9014 - lr: 0.1000\n",
      "Epoch 13/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.4894 - accuracy: 0.8988 - val_loss: 0.4586 - val_accuracy: 0.9040 - lr: 0.1000\n",
      "Epoch 14/400\n",
      "1506/1506 [==============================] - 130s 87ms/step - loss: 0.4819 - accuracy: 0.8999 - val_loss: 0.4732 - val_accuracy: 0.8948 - lr: 0.1000\n",
      "Epoch 15/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.4755 - accuracy: 0.9009 - val_loss: 0.4514 - val_accuracy: 0.9047 - lr: 0.1000\n",
      "Epoch 16/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.4720 - accuracy: 0.9007 - val_loss: 0.4324 - val_accuracy: 0.9122 - lr: 0.1000\n",
      "Epoch 17/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.4662 - accuracy: 0.9021 - val_loss: 0.4476 - val_accuracy: 0.9019 - lr: 0.1000\n",
      "Epoch 18/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.4611 - accuracy: 0.9022 - val_loss: 0.4343 - val_accuracy: 0.9086 - lr: 0.1000\n",
      "Epoch 19/400\n",
      "1506/1506 [==============================] - 138s 92ms/step - loss: 0.4565 - accuracy: 0.9042 - val_loss: 0.4246 - val_accuracy: 0.9109 - lr: 0.1000\n",
      "Epoch 20/400\n",
      "1506/1506 [==============================] - 134s 89ms/step - loss: 0.4531 - accuracy: 0.9049 - val_loss: 0.4286 - val_accuracy: 0.9085 - lr: 0.1000\n",
      "Epoch 21/400\n",
      "1506/1506 [==============================] - 133s 88ms/step - loss: 0.4465 - accuracy: 0.9071 - val_loss: 0.4051 - val_accuracy: 0.9200 - lr: 0.1000\n",
      "Epoch 22/400\n",
      "1506/1506 [==============================] - 144s 95ms/step - loss: 0.4440 - accuracy: 0.9062 - val_loss: 0.4325 - val_accuracy: 0.9034 - lr: 0.1000\n",
      "Epoch 23/400\n",
      "1506/1506 [==============================] - 138s 92ms/step - loss: 0.4405 - accuracy: 0.9063 - val_loss: 0.4095 - val_accuracy: 0.9138 - lr: 0.1000\n",
      "Epoch 24/400\n",
      "1506/1506 [==============================] - 135s 90ms/step - loss: 0.4386 - accuracy: 0.9068 - val_loss: 0.4077 - val_accuracy: 0.9146 - lr: 0.1000\n",
      "Epoch 25/400\n",
      "1506/1506 [==============================] - 137s 91ms/step - loss: 0.4344 - accuracy: 0.9076 - val_loss: 0.4116 - val_accuracy: 0.9120 - lr: 0.1000\n",
      "Epoch 26/400\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.9087\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "1506/1506 [==============================] - 135s 90ms/step - loss: 0.4308 - accuracy: 0.9087 - val_loss: 0.4140 - val_accuracy: 0.9096 - lr: 0.1000\n",
      "Epoch 27/400\n",
      "1506/1506 [==============================] - 135s 90ms/step - loss: 0.4215 - accuracy: 0.9108 - val_loss: 0.4015 - val_accuracy: 0.9123 - lr: 0.0500\n",
      "Epoch 28/400\n",
      "1506/1506 [==============================] - 135s 90ms/step - loss: 0.4204 - accuracy: 0.9118 - val_loss: 0.4049 - val_accuracy: 0.9107 - lr: 0.0500\n",
      "Epoch 29/400\n",
      "1506/1506 [==============================] - 137s 91ms/step - loss: 0.4178 - accuracy: 0.9123 - val_loss: 0.3826 - val_accuracy: 0.9221 - lr: 0.0500\n",
      "Epoch 30/400\n",
      "1506/1506 [==============================] - 135s 90ms/step - loss: 0.4159 - accuracy: 0.9128 - val_loss: 0.3854 - val_accuracy: 0.9203 - lr: 0.0500\n",
      "Epoch 31/400\n",
      "1506/1506 [==============================] - 133s 88ms/step - loss: 0.4148 - accuracy: 0.9129 - val_loss: 0.3906 - val_accuracy: 0.9164 - lr: 0.0500\n",
      "Epoch 32/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.4140 - accuracy: 0.9137 - val_loss: 0.4058 - val_accuracy: 0.9091 - lr: 0.0500\n",
      "Epoch 33/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.4136 - accuracy: 0.9129 - val_loss: 0.4003 - val_accuracy: 0.9123 - lr: 0.0500\n",
      "Epoch 34/400\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.9134\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.4118 - accuracy: 0.9134 - val_loss: 0.3924 - val_accuracy: 0.9150 - lr: 0.0500\n",
      "Epoch 35/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.4069 - accuracy: 0.9151 - val_loss: 0.3864 - val_accuracy: 0.9168 - lr: 0.0250\n",
      "Epoch 36/400\n",
      "1506/1506 [==============================] - 134s 89ms/step - loss: 0.4057 - accuracy: 0.9161 - val_loss: 0.3914 - val_accuracy: 0.9154 - lr: 0.0250\n",
      "Epoch 37/400\n",
      "1506/1506 [==============================] - 134s 89ms/step - loss: 0.4061 - accuracy: 0.9158 - val_loss: 0.3825 - val_accuracy: 0.9199 - lr: 0.0250\n",
      "Epoch 38/400\n",
      "1506/1506 [==============================] - 133s 89ms/step - loss: 0.4047 - accuracy: 0.9148 - val_loss: 0.3833 - val_accuracy: 0.9190 - lr: 0.0250\n",
      "Epoch 39/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.4044 - accuracy: 0.9155 - val_loss: 0.3849 - val_accuracy: 0.9187 - lr: 0.0250\n",
      "Epoch 40/400\n",
      "1506/1506 [==============================] - 134s 89ms/step - loss: 0.4033 - accuracy: 0.9158 - val_loss: 0.3776 - val_accuracy: 0.9217 - lr: 0.0250\n",
      "Epoch 41/400\n",
      "1506/1506 [==============================] - 133s 89ms/step - loss: 0.4037 - accuracy: 0.9158 - val_loss: 0.3847 - val_accuracy: 0.9174 - lr: 0.0250\n",
      "Epoch 42/400\n",
      "1506/1506 [==============================] - 133s 88ms/step - loss: 0.4040 - accuracy: 0.9158 - val_loss: 0.3830 - val_accuracy: 0.9183 - lr: 0.0250\n",
      "Epoch 43/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.4022 - accuracy: 0.9166 - val_loss: 0.3791 - val_accuracy: 0.9198 - lr: 0.0250\n",
      "Epoch 44/400\n",
      "1506/1506 [==============================] - 132s 88ms/step - loss: 0.4024 - accuracy: 0.9160 - val_loss: 0.3924 - val_accuracy: 0.9135 - lr: 0.0250\n",
      "Epoch 45/400\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.9165\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.4012 - accuracy: 0.9165 - val_loss: 0.3826 - val_accuracy: 0.9179 - lr: 0.0250\n",
      "Epoch 46/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.3992 - accuracy: 0.9172 - val_loss: 0.3770 - val_accuracy: 0.9209 - lr: 0.0125\n",
      "Epoch 47/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.3992 - accuracy: 0.9170 - val_loss: 0.3845 - val_accuracy: 0.9170 - lr: 0.0125\n",
      "Epoch 48/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.3980 - accuracy: 0.9178 - val_loss: 0.3814 - val_accuracy: 0.9187 - lr: 0.0125\n",
      "Epoch 49/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.3994 - accuracy: 0.9180 - val_loss: 0.3773 - val_accuracy: 0.9202 - lr: 0.0125\n",
      "Epoch 50/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.3982 - accuracy: 0.9179 - val_loss: 0.3845 - val_accuracy: 0.9170 - lr: 0.0125\n",
      "Epoch 51/400\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3990 - accuracy: 0.9177\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.3990 - accuracy: 0.9177 - val_loss: 0.3803 - val_accuracy: 0.9189 - lr: 0.0125\n",
      "Epoch 52/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.3966 - accuracy: 0.9191 - val_loss: 0.3764 - val_accuracy: 0.9210 - lr: 0.0063\n",
      "Epoch 53/400\n",
      "1506/1506 [==============================] - 132s 87ms/step - loss: 0.3971 - accuracy: 0.9181 - val_loss: 0.3785 - val_accuracy: 0.9192 - lr: 0.0063\n",
      "Epoch 54/400\n",
      "1506/1506 [==============================] - 128s 85ms/step - loss: 0.3969 - accuracy: 0.9178 - val_loss: 0.3848 - val_accuracy: 0.9164 - lr: 0.0063\n",
      "Epoch 55/400\n",
      "1506/1506 [==============================] - 129s 86ms/step - loss: 0.3973 - accuracy: 0.9167 - val_loss: 0.3852 - val_accuracy: 0.9163 - lr: 0.0063\n",
      "Epoch 56/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.3971 - accuracy: 0.9165 - val_loss: 0.3739 - val_accuracy: 0.9214 - lr: 0.0063\n",
      "Epoch 57/400\n",
      "1506/1506 [==============================] - 129s 86ms/step - loss: 0.3975 - accuracy: 0.9176 - val_loss: 0.3815 - val_accuracy: 0.9180 - lr: 0.0063\n",
      "Epoch 58/400\n",
      "1506/1506 [==============================] - 128s 85ms/step - loss: 0.3984 - accuracy: 0.9168 - val_loss: 0.3780 - val_accuracy: 0.9195 - lr: 0.0063\n",
      "Epoch 59/400\n",
      "1506/1506 [==============================] - 128s 85ms/step - loss: 0.3975 - accuracy: 0.9174 - val_loss: 0.3785 - val_accuracy: 0.9194 - lr: 0.0063\n",
      "Epoch 60/400\n",
      "1506/1506 [==============================] - 128s 85ms/step - loss: 0.3969 - accuracy: 0.9173 - val_loss: 0.3796 - val_accuracy: 0.9194 - lr: 0.0063\n",
      "Epoch 61/400\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.9177\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0031250000465661287.\n",
      "1506/1506 [==============================] - 128s 85ms/step - loss: 0.3969 - accuracy: 0.9177 - val_loss: 0.3764 - val_accuracy: 0.9199 - lr: 0.0063\n",
      "Epoch 62/400\n",
      "1506/1506 [==============================] - 128s 85ms/step - loss: 0.3961 - accuracy: 0.9179 - val_loss: 0.3832 - val_accuracy: 0.9168 - lr: 0.0031\n",
      "Epoch 63/400\n",
      "1506/1506 [==============================] - 128s 85ms/step - loss: 0.3963 - accuracy: 0.9177 - val_loss: 0.3723 - val_accuracy: 0.9219 - lr: 0.0031\n",
      "Epoch 64/400\n",
      "1506/1506 [==============================] - 129s 86ms/step - loss: 0.3971 - accuracy: 0.9174 - val_loss: 0.3793 - val_accuracy: 0.9188 - lr: 0.0031\n",
      "Epoch 65/400\n",
      "1506/1506 [==============================] - 130s 86ms/step - loss: 0.3970 - accuracy: 0.9182 - val_loss: 0.3778 - val_accuracy: 0.9199 - lr: 0.0031\n",
      "Epoch 66/400\n",
      "1506/1506 [==============================] - 129s 86ms/step - loss: 0.3944 - accuracy: 0.9198 - val_loss: 0.3784 - val_accuracy: 0.9196 - lr: 0.0031\n",
      "Epoch 67/400\n",
      "1506/1506 [==============================] - 131s 87ms/step - loss: 0.3965 - accuracy: 0.9174 - val_loss: 0.3745 - val_accuracy: 0.9215 - lr: 0.0031\n",
      "Epoch 68/400\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3958 - accuracy: 0.9179\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.0015625000232830644.\n",
      "1506/1506 [==============================] - 134s 89ms/step - loss: 0.3958 - accuracy: 0.9179 - val_loss: 0.3725 - val_accuracy: 0.9226 - lr: 0.0031\n",
      "Epoch 69/400\n",
      "1506/1506 [==============================] - 137s 91ms/step - loss: 0.3972 - accuracy: 0.9175 - val_loss: 0.3837 - val_accuracy: 0.9177 - lr: 0.0016\n",
      "Epoch 70/400\n",
      "1506/1506 [==============================] - 130s 87ms/step - loss: 0.3976 - accuracy: 0.9172 - val_loss: 0.3827 - val_accuracy: 0.9177 - lr: 0.0016\n",
      "Epoch 71/400\n",
      "1506/1506 [==============================] - 132s 87ms/step - loss: 0.3964 - accuracy: 0.9178 - val_loss: 0.3765 - val_accuracy: 0.9205 - lr: 0.0016\n",
      "Epoch 72/400\n",
      "1506/1506 [==============================] - 135s 90ms/step - loss: 0.3958 - accuracy: 0.9182 - val_loss: 0.3786 - val_accuracy: 0.9189 - lr: 0.0016\n",
      "Epoch 73/400\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3954 - accuracy: 0.9184\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0007812500116415322.\n",
      "1506/1506 [==============================] - 136s 90ms/step - loss: 0.3954 - accuracy: 0.9184 - val_loss: 0.3751 - val_accuracy: 0.9206 - lr: 0.0016\n",
      "Epoch 74/400\n",
      "1506/1506 [==============================] - 134s 89ms/step - loss: 0.3955 - accuracy: 0.9179 - val_loss: 0.3761 - val_accuracy: 0.9205 - lr: 7.8125e-04\n",
      "Epoch 75/400\n",
      "1506/1506 [==============================] - 133s 88ms/step - loss: 0.3955 - accuracy: 0.9186 - val_loss: 0.3786 - val_accuracy: 0.9191 - lr: 7.8125e-04\n",
      "Epoch 00075: early stopping\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=classWeight,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Графіки тренування моделі\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m----> 5\u001b[0m plt\u001b[39m.\u001b[39mplot(H\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[39m.\u001b[39mplot(H\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m plt\u001b[39m.\u001b[39mplot(H\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_acc\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Графіки тренування моделі\n",
    "plt.figure()\n",
    "plt.plot(H.history['loss'], label='train_loss')\n",
    "plt.plot(H.history['val_loss'], label='val_loss')\n",
    "plt.plot(H.history['accuracy'], label='train_acc')\n",
    "plt.plot(H.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: Ukrainian_OCR_SavedModel\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Ukrainian_OCR_SavedModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Ukrainian_OCR_tf_2.1.h5',save_format=\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('Ukrainian_OCR_extended_Resnet_with_blure_and_aug_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# evaluate the network\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[INFO] evaluating network...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(testX, batch_size\u001b[39m=\u001b[39;49mBS)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(classification_report(testY\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     11\u001b[0m \tpredictions\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), target_names\u001b[39m=\u001b[39mlabelNames))\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# define the list of label names\n",
    "\n",
    "labelNames = \"АБВГҐДЕЄЖЗИІЇЙКЛМНОПРСТУФХЦЧШЩЬЮЯабвгґдеєжзиіїйклмнопрстуфхцчшщьюя1234567890№%@,.?:;\\\"!()-'\"\n",
    "labelNames = [l for l in labelNames]\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "from imutils import build_montages\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "# randomly select a few testing characters\n",
    "\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(66,)):\n",
    "    probs = model.predict(testX[np.newaxis, i])\n",
    "    prediction = probs.argmax(axis=1)\n",
    "    label = labelNames[prediction[0]]\n",
    "    output += label\n",
    "    image = (testX[i] * 255).astype(\"uint8\")\n",
    "    color = (0, 255, 0)\n",
    "    color = (0, 0, 255)\n",
    "    image = cv2.merge([image] * 3)\n",
    "    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "    img_pil = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=34)\n",
    "    draw.text((5, 20), label, font=font, fill=color)\n",
    "    image = np.array(img_pil)\n",
    "    images.append(image)\n",
    "  \n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "cv2.imshow('q',montage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'гЛЄРНезцХВЙжЮЕйткЗПццЗнТчХЧСрОРаФЬЩҐтИЖйИАОтЯмрЛЩ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
