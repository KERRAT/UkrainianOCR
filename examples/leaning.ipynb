{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset(datasetPath):\n",
    "\n",
    "  # List for storing data\n",
    "  data = []\n",
    "  \n",
    "  # List for storing labels\n",
    "  labels = []\n",
    "  \n",
    "  for row in open(datasetPath): #Openfile and start reading each row\n",
    "    #Split the row at every comma\n",
    "    row = row.split(\",\")\n",
    "    \n",
    "    #row[0] contains label\n",
    "    label = int(row[0])\n",
    "    \n",
    "    #Other all collumns contains pixel values make a saperate array for that\n",
    "    image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "    \n",
    "    #Reshaping image to 28 x 28 pixels\n",
    "    image = image.reshape((28, 28))\n",
    "    \n",
    "    #append image to data\n",
    "    data.append(image)\n",
    "    \n",
    "    #append label to labels\n",
    "    labels.append(label)\n",
    "    \n",
    "  #Converting data to numpy array of type float32\n",
    "  data = np.array(data, dtype='float32')\n",
    "  \n",
    "  #Converting labels to type int\n",
    "  labels = np.array(labels, dtype=\"int\")\n",
    "  \n",
    "  return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Data, Labels) = load_dataset(\"../data/uaset_without_coloumns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# However, the architecture we're using is designed for 32x32 images,\n",
    "# So we need to resize them to 32x32\n",
    "\n",
    "Data = [cv2.resize(image, (32, 32)) for image in Data]\n",
    "Data = np.array(Data, dtype=\"float32\")\n",
    "\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "\n",
    "Data = np.expand_dims(Data, axis=-1)\n",
    "Data /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "le = LabelBinarizer()\n",
    "Labels = le.fit_transform(Labels)\n",
    "\n",
    "counts = Labels.sum(axis=0)\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals = Labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "  classWeight[i] = classTotals.max() / classTotals[i]\n",
    "  \n",
    "(trainX, testX, trainY, testY) = train_test_split(Data,\n",
    "\tLabels, test_size=0.20, stratify=Labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "rotation_range=10,\n",
    "zoom_range=0.05,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "shear_range=0.15,\n",
    "horizontal_flip=False,\n",
    "fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import add\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and apply BN\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\n",
    "\t\t# check if we are utilizing the CIFAR dataset\n",
    "\t\tif dataset == \"cifar\":\n",
    "\t\t\t# apply a single CONV layer\n",
    "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
    "\t\telif dataset == \"tiny_imagenet\":\n",
    "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\t\tmomentum=bnMom)(x)\n",
    "\t\t\tx = Activation(\"relu\")(x)\n",
    "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
    "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\t\t# loop over the number of stages\n",
    "\t\tfor i in range(0, len(stages)):\n",
    "\t\t\t# initialize the stride, then apply a residual module\n",
    "\t\t\t# used to reduce the spatial size of the input volume\n",
    "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t\t# loop over the number of layers in the stage\n",
    "\t\t\tfor j in range(0, stages[i] - 1):\n",
    "\t\t\t\t# apply a ResNet module\n",
    "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "INIT_LR = 1e-1\n",
    "BS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "124/124 [==============================] - 17s 100ms/step - loss: 4.2956 - accuracy: 0.0368 - val_loss: 4.0367 - val_accuracy: 0.0491\n",
      "Epoch 2/50\n",
      "124/124 [==============================] - 11s 86ms/step - loss: 3.8510 - accuracy: 0.1403 - val_loss: 3.5277 - val_accuracy: 0.1841\n",
      "Epoch 3/50\n",
      "124/124 [==============================] - 11s 87ms/step - loss: 3.2937 - accuracy: 0.2753 - val_loss: 3.0321 - val_accuracy: 0.3089\n",
      "Epoch 4/50\n",
      "124/124 [==============================] - 11s 85ms/step - loss: 2.7447 - accuracy: 0.4339 - val_loss: 2.4585 - val_accuracy: 0.4092\n",
      "Epoch 5/50\n",
      "124/124 [==============================] - 11s 86ms/step - loss: 2.2651 - accuracy: 0.5642 - val_loss: 2.4545 - val_accuracy: 0.3816\n",
      "Epoch 6/50\n",
      "124/124 [==============================] - 11s 86ms/step - loss: 1.8736 - accuracy: 0.6801 - val_loss: 1.6282 - val_accuracy: 0.7352\n",
      "Epoch 7/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 1.5597 - accuracy: 0.7577 - val_loss: 1.4799 - val_accuracy: 0.7753\n",
      "Epoch 8/50\n",
      "124/124 [==============================] - 11s 89ms/step - loss: 1.3214 - accuracy: 0.8138 - val_loss: 1.2872 - val_accuracy: 0.7437\n",
      "Epoch 9/50\n",
      "124/124 [==============================] - 11s 90ms/step - loss: 1.1626 - accuracy: 0.8368 - val_loss: 1.2836 - val_accuracy: 0.7322\n",
      "Epoch 10/50\n",
      "124/124 [==============================] - 11s 87ms/step - loss: 1.0267 - accuracy: 0.8561 - val_loss: 1.0363 - val_accuracy: 0.8029\n",
      "Epoch 11/50\n",
      "124/124 [==============================] - 11s 87ms/step - loss: 0.9420 - accuracy: 0.8683 - val_loss: 0.9754 - val_accuracy: 0.8200\n",
      "Epoch 12/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.8684 - accuracy: 0.8833 - val_loss: 0.8158 - val_accuracy: 0.8771\n",
      "Epoch 13/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.8077 - accuracy: 0.8976 - val_loss: 0.9218 - val_accuracy: 0.8345\n",
      "Epoch 14/50\n",
      "124/124 [==============================] - 11s 89ms/step - loss: 0.7640 - accuracy: 0.8995 - val_loss: 0.7291 - val_accuracy: 0.9017\n",
      "Epoch 15/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.7358 - accuracy: 0.9068 - val_loss: 0.7714 - val_accuracy: 0.8671\n",
      "Epoch 16/50\n",
      "124/124 [==============================] - 11s 87ms/step - loss: 0.6954 - accuracy: 0.9178 - val_loss: 0.7614 - val_accuracy: 0.8696\n",
      "Epoch 17/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.6700 - accuracy: 0.9192 - val_loss: 1.0679 - val_accuracy: 0.7367\n",
      "Epoch 18/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.6459 - accuracy: 0.9235 - val_loss: 0.5997 - val_accuracy: 0.9308\n",
      "Epoch 19/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.6296 - accuracy: 0.9249 - val_loss: 0.8291 - val_accuracy: 0.8410\n",
      "Epoch 20/50\n",
      "124/124 [==============================] - 11s 89ms/step - loss: 0.6102 - accuracy: 0.9322 - val_loss: 0.6798 - val_accuracy: 0.8932\n",
      "Epoch 21/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5891 - accuracy: 0.9365 - val_loss: 0.5496 - val_accuracy: 0.9358\n",
      "Epoch 22/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5732 - accuracy: 0.9361 - val_loss: 0.5684 - val_accuracy: 0.9203\n",
      "Epoch 23/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5702 - accuracy: 0.9365 - val_loss: 0.5409 - val_accuracy: 0.9278\n",
      "Epoch 24/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5597 - accuracy: 0.9378 - val_loss: 0.7239 - val_accuracy: 0.8581\n",
      "Epoch 25/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5435 - accuracy: 0.9441 - val_loss: 0.5971 - val_accuracy: 0.9107\n",
      "Epoch 26/50\n",
      "124/124 [==============================] - 11s 91ms/step - loss: 0.5353 - accuracy: 0.9434 - val_loss: 0.5284 - val_accuracy: 0.9348\n",
      "Epoch 27/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5252 - accuracy: 0.9456 - val_loss: 0.5890 - val_accuracy: 0.9077\n",
      "Epoch 28/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5203 - accuracy: 0.9464 - val_loss: 0.5072 - val_accuracy: 0.9388\n",
      "Epoch 29/50\n",
      "124/124 [==============================] - 11s 89ms/step - loss: 0.5236 - accuracy: 0.9471 - val_loss: 0.6374 - val_accuracy: 0.8887\n",
      "Epoch 30/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.5160 - accuracy: 0.9451 - val_loss: 0.5295 - val_accuracy: 0.9243\n",
      "Epoch 31/50\n",
      "124/124 [==============================] - 12s 99ms/step - loss: 0.5079 - accuracy: 0.9474 - val_loss: 0.5939 - val_accuracy: 0.8972\n",
      "Epoch 32/50\n",
      "124/124 [==============================] - 12s 98ms/step - loss: 0.5056 - accuracy: 0.9499 - val_loss: 0.5743 - val_accuracy: 0.9092\n",
      "Epoch 33/50\n",
      "124/124 [==============================] - 16s 127ms/step - loss: 0.5012 - accuracy: 0.9499 - val_loss: 0.5239 - val_accuracy: 0.9228\n",
      "Epoch 34/50\n",
      "124/124 [==============================] - 15s 121ms/step - loss: 0.4960 - accuracy: 0.9499 - val_loss: 0.5335 - val_accuracy: 0.9248\n",
      "Epoch 35/50\n",
      "124/124 [==============================] - 15s 115ms/step - loss: 0.4850 - accuracy: 0.9569 - val_loss: 0.4910 - val_accuracy: 0.9383\n",
      "Epoch 36/50\n",
      "124/124 [==============================] - 11s 91ms/step - loss: 0.4851 - accuracy: 0.9503 - val_loss: 0.5902 - val_accuracy: 0.9022\n",
      "Epoch 37/50\n",
      "124/124 [==============================] - 12s 95ms/step - loss: 0.4790 - accuracy: 0.9554 - val_loss: 0.4890 - val_accuracy: 0.9403\n",
      "Epoch 38/50\n",
      "124/124 [==============================] - 11s 89ms/step - loss: 0.4819 - accuracy: 0.9533 - val_loss: 0.5893 - val_accuracy: 0.9027\n",
      "Epoch 39/50\n",
      "124/124 [==============================] - 12s 93ms/step - loss: 0.4752 - accuracy: 0.9525 - val_loss: 0.5001 - val_accuracy: 0.9383\n",
      "Epoch 40/50\n",
      "124/124 [==============================] - 17s 134ms/step - loss: 0.4746 - accuracy: 0.9565 - val_loss: 0.5154 - val_accuracy: 0.9283\n",
      "Epoch 41/50\n",
      "124/124 [==============================] - 14s 113ms/step - loss: 0.4716 - accuracy: 0.9544 - val_loss: 0.5206 - val_accuracy: 0.9162\n",
      "Epoch 42/50\n",
      "124/124 [==============================] - 11s 90ms/step - loss: 0.4691 - accuracy: 0.9544 - val_loss: 0.5443 - val_accuracy: 0.9072\n",
      "Epoch 43/50\n",
      "124/124 [==============================] - 11s 91ms/step - loss: 0.4653 - accuracy: 0.9546 - val_loss: 0.5381 - val_accuracy: 0.9142\n",
      "Epoch 44/50\n",
      "124/124 [==============================] - 11s 90ms/step - loss: 0.4596 - accuracy: 0.9599 - val_loss: 0.5526 - val_accuracy: 0.9057\n",
      "Epoch 45/50\n",
      "124/124 [==============================] - 16s 128ms/step - loss: 0.4608 - accuracy: 0.9560 - val_loss: 0.5237 - val_accuracy: 0.9218\n",
      "Epoch 46/50\n",
      "124/124 [==============================] - 17s 140ms/step - loss: 0.4606 - accuracy: 0.9566 - val_loss: 0.5212 - val_accuracy: 0.9188\n",
      "Epoch 47/50\n",
      "124/124 [==============================] - 11s 92ms/step - loss: 0.4500 - accuracy: 0.9603 - val_loss: 0.5043 - val_accuracy: 0.9248\n",
      "Epoch 48/50\n",
      "124/124 [==============================] - 11s 89ms/step - loss: 0.4557 - accuracy: 0.9565 - val_loss: 0.4912 - val_accuracy: 0.9368\n",
      "Epoch 49/50\n",
      "124/124 [==============================] - 11s 89ms/step - loss: 0.4506 - accuracy: 0.9593 - val_loss: 0.5424 - val_accuracy: 0.9137\n",
      "Epoch 50/50\n",
      "124/124 [==============================] - 11s 88ms/step - loss: 0.4425 - accuracy: 0.9627 - val_loss: 0.5480 - val_accuracy: 0.9112\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "aug.flow(trainX, trainY, batch_size=BS),\n",
    "validation_data=(testX, testY),\n",
    "steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "class_weight=classWeight,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Ukrainian_OCR_Resnet.h5',save_format=\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('Ukrainian_OCR_Resnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "32/32 [==============================] - 1s 22ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           А       1.00      1.00      1.00        30\n",
      "           Б       1.00      1.00      1.00        30\n",
      "           В       0.86      1.00      0.92        30\n",
      "           Г       0.85      0.97      0.91        30\n",
      "           Ґ       0.83      0.97      0.89        30\n",
      "           Д       0.91      0.97      0.94        30\n",
      "           Е       1.00      1.00      1.00        30\n",
      "           Є       0.86      1.00      0.93        31\n",
      "           Ж       0.81      1.00      0.90        30\n",
      "           З       0.86      1.00      0.92        30\n",
      "           И       0.78      1.00      0.87        31\n",
      "           І       0.86      1.00      0.92        30\n",
      "           Ї       0.88      1.00      0.94        30\n",
      "           Й       0.78      1.00      0.87        31\n",
      "           К       0.83      0.97      0.89        30\n",
      "           Л       0.81      0.97      0.88        30\n",
      "           М       0.83      0.97      0.89        30\n",
      "           Н       0.86      1.00      0.92        30\n",
      "           О       0.94      0.97      0.95        30\n",
      "           П       0.91      1.00      0.95        30\n",
      "           Р       1.00      1.00      1.00        30\n",
      "           С       0.88      1.00      0.94        30\n",
      "           Т       0.86      0.97      0.91        31\n",
      "           У       0.88      0.97      0.92        30\n",
      "           Ф       0.93      0.93      0.93        30\n",
      "           Х       0.83      0.97      0.89        30\n",
      "           Ц       0.89      1.00      0.94        31\n",
      "           Ч       0.94      0.97      0.95        30\n",
      "           Ш       0.75      1.00      0.86        30\n",
      "           Щ       0.81      1.00      0.90        30\n",
      "           Ь       0.77      1.00      0.87        30\n",
      "           Ю       0.85      0.97      0.91        30\n",
      "           Я       0.77      1.00      0.87        30\n",
      "           а       1.00      0.93      0.97        30\n",
      "           б       1.00      1.00      1.00        30\n",
      "           в       1.00      0.83      0.91        30\n",
      "           г       0.96      0.84      0.90        31\n",
      "           ґ       0.96      0.81      0.88        31\n",
      "           д       0.96      0.90      0.93        30\n",
      "           е       1.00      1.00      1.00        30\n",
      "           є       1.00      0.80      0.89        30\n",
      "           ж       1.00      0.77      0.87        30\n",
      "           з       1.00      0.84      0.91        31\n",
      "           и       0.96      0.74      0.84        31\n",
      "           і       1.00      0.83      0.91        30\n",
      "           ї       1.00      0.87      0.93        31\n",
      "           й       0.95      0.70      0.81        30\n",
      "           к       1.00      0.81      0.89        31\n",
      "           л       1.00      0.77      0.87        30\n",
      "           м       0.96      0.80      0.87        30\n",
      "           н       1.00      0.87      0.93        30\n",
      "           о       0.94      0.94      0.94        31\n",
      "           п       1.00      0.93      0.97        30\n",
      "           р       1.00      1.00      1.00        30\n",
      "           с       0.96      0.87      0.91        30\n",
      "           т       0.96      0.84      0.90        31\n",
      "           у       1.00      0.83      0.91        30\n",
      "           ф       0.93      0.93      0.93        30\n",
      "           х       0.96      0.81      0.88        31\n",
      "           ц       0.96      0.80      0.87        30\n",
      "           ч       0.93      0.93      0.93        30\n",
      "           ш       0.95      0.70      0.81        30\n",
      "           щ       1.00      0.70      0.82        30\n",
      "           ь       1.00      0.70      0.82        30\n",
      "           ю       0.96      0.83      0.89        30\n",
      "           я       1.00      0.70      0.82        30\n",
      "\n",
      "    accuracy                           0.91      1994\n",
      "   macro avg       0.92      0.91      0.91      1994\n",
      "weighted avg       0.92      0.91      0.91      1994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# define the list of label names\n",
    "\n",
    "labelNames = 'АБВГҐДЕЄЖЗИІЇЙКЛМНОПРСТУФХЦЧШЩЬЮЯабвгґдеєжзиіїйклмнопрстуфхцчшщьюя'\n",
    "labelNames = [l for l in labelNames]\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "from imutils import build_montages\n",
    "\n",
    "\n",
    "images = []\n",
    "# randomly select a few testing characters\n",
    "\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(66,)):\n",
    "  probs = model.predict(testX[np.newaxis, i])\n",
    "  prediction = probs.argmax(axis=1)\n",
    "  label = labelNames[prediction[0]]\n",
    "  output+=label\n",
    "  image = (testX[i] * 255).astype(\"uint8\")\n",
    "  color = (0, 255, 0)\n",
    "  color = (0, 0, 255)\n",
    "  image = cv2.merge([image] * 3)\n",
    "  image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "  cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_COMPLEX, 0.75,\n",
    "  color, 2)\n",
    "  images.append(image)\n",
    "  \n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "cv2.imshow('q',montage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'гЛЄРНезцХВЙжЮЕйткЗПццЗнТчХЧСрОРаФЬЩҐтИЖйИАОтЯмрЛЩ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
