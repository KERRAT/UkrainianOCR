{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset(datasetPath):\n",
    "\n",
    "  # List for storing data\n",
    "  data = []\n",
    "  \n",
    "  # List for storing labels\n",
    "  labels = []\n",
    "  \n",
    "  for row in open(datasetPath): #Openfile and start reading each row\n",
    "    #Split the row at every comma\n",
    "    row = row.split(\",\")\n",
    "    \n",
    "    #row[0] contains label\n",
    "    label = int(row[0])\n",
    "    \n",
    "    #Other all collumns contains pixel values make a saperate array for that\n",
    "    image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "    \n",
    "    #Reshaping image to 28 x 28 pixels\n",
    "    image = image.reshape((32, 32))\n",
    "    \n",
    "    #append image to data\n",
    "    data.append(image)\n",
    "    \n",
    "    #append label to labels\n",
    "    labels.append(label)\n",
    "    \n",
    "  #Converting data to numpy array of type float32\n",
    "  data = np.array(data, dtype='float32')\n",
    "  \n",
    "  #Converting labels to type int\n",
    "  labels = np.array(labels, dtype=\"int\")\n",
    "  \n",
    "  return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Data, Labels) = load_dataset(\"../data/uaset_without_columns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Data, Labels) = load_dataset(\"../data/uaset_extended_with_scale_factor_and_3x3_blure.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# However, the architecture we're using is designed for 32x32 images,\n",
    "# So we need to resize them to 32x32\n",
    "\n",
    "Data = [cv2.resize(image, (32, 32)) for image in Data]\n",
    "Data = np.array(Data, dtype=\"float32\")\n",
    "\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "\n",
    "Data = np.expand_dims(Data, axis=-1)\n",
    "Data /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "le = LabelBinarizer()\n",
    "Labels = le.fit_transform(Labels)\n",
    "\n",
    "counts = Labels.sum(axis=0)\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals = Labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "  classWeight[i] = classTotals.max() / classTotals[i]\n",
    "  \n",
    "(trainX, testX, trainY, testY) = train_test_split(Data,\n",
    "\tLabels, test_size=0.30, stratify=Labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "rotation_range=15,\n",
    "zoom_range=0.07,\n",
    "width_shift_range=0.16,\n",
    "height_shift_range=0.16,\n",
    "shear_range=0.22,\n",
    "horizontal_flip=False,\n",
    "fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import add\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and apply BN\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\n",
    "\t\t# check if we are utilizing the CIFAR dataset\n",
    "\t\tif dataset == \"cifar\":\n",
    "\t\t\t# apply a single CONV layer\n",
    "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
    "\t\telif dataset == \"tiny_imagenet\":\n",
    "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\t\tmomentum=bnMom)(x)\n",
    "\t\t\tx = Activation(\"relu\")(x)\n",
    "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
    "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\t\t# loop over the number of stages\n",
    "\t\tfor i in range(0, len(stages)):\n",
    "\t\t\t# initialize the stride, then apply a residual module\n",
    "\t\t\t# used to reduce the spatial size of the input volume\n",
    "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t\t# loop over the number of layers in the stage\n",
    "\t\t\tfor j in range(0, stages[i] - 1):\n",
    "\t\t\t\t# apply a ResNet module\n",
    "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "INIT_LR = 1e-1\n",
    "BS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 32, 32, 1)   4           ['input_2[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 32, 32, 64)   576         ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 32, 32, 64)  256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 32, 32, 16)   1024        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 32, 32, 16)  64          ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 32, 32, 16)   2304        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 32, 32, 16)  64          ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 32, 32, 64)   1024        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 32, 32, 64)   4096        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 32, 32, 64)   0           ['conv2d_34[0][0]',              \n",
      "                                                                  'conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 32, 32, 64)  256         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 32, 32, 16)   1024        ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 32, 32, 16)  64          ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 32, 32, 16)   2304        ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 32, 32, 16)  64          ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 32, 32, 64)   1024        ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 32, 32, 64)   0           ['conv2d_38[0][0]',              \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 32, 32, 64)  256         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 32, 32, 16)   1024        ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 32, 32, 16)  64          ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 32, 32, 16)   2304        ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 32, 32, 16)  64          ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 32, 32, 64)   1024        ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 32, 32, 64)   0           ['conv2d_41[0][0]',              \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 32, 32, 64)  256         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 32, 32, 32)   2048        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 32, 32, 32)  128         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 16, 16, 32)   9216        ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 16, 16, 32)  128         ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 16, 16, 128)  4096        ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 16, 16, 128)  8192        ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 128)  0           ['conv2d_44[0][0]',              \n",
      "                                                                  'conv2d_45[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 16, 16, 128)  512        ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 16, 16, 32)   4096        ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 16, 16, 32)  128         ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 16, 16, 32)   9216        ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 16, 16, 32)  128         ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 16, 16, 128)  4096        ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 128)  0           ['conv2d_48[0][0]',              \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 16, 16, 128)  512        ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 16, 16, 32)   4096        ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 16, 16, 32)  128         ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 16, 16, 32)   9216        ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 16, 16, 32)  128         ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 16, 16, 128)  4096        ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 16, 16, 128)  0           ['conv2d_51[0][0]',              \n",
      "                                                                  'add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 16, 16, 128)  512        ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 16, 16, 64)   8192        ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 16, 16, 64)  256         ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 8, 8, 64)     36864       ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 8, 8, 64)    256         ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 8, 8, 256)    16384       ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 8, 8, 256)    32768       ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 256)    0           ['conv2d_54[0][0]',              \n",
      "                                                                  'conv2d_55[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 8, 8, 256)   1024        ['add_15[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 8, 8, 64)     16384       ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 8, 8, 64)     36864       ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 8, 8, 256)    16384       ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 256)    0           ['conv2d_58[0][0]',              \n",
      "                                                                  'add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 8, 8, 256)   1024        ['add_16[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 8, 8, 64)     16384       ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 8, 8, 64)     36864       ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 8, 8, 64)    256         ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 8, 8, 256)    16384       ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 256)    0           ['conv2d_61[0][0]',              \n",
      "                                                                  'add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 8, 8, 256)   1024        ['add_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 1, 1, 256)   0           ['activation_56[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 256)          0           ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 90)           23130       ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 90)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 341,022\n",
      "Trainable params: 336,860\n",
      "Non-trainable params: 4,162\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Додайте колбеки\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=18, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=6, min_lr=0.0001, verbose=1)\n",
    "\n",
    "callbacks_list = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1506/1506 [==============================] - 146s 94ms/step - loss: 2.4146 - accuracy: 0.4583 - val_loss: 1.2978 - val_accuracy: 0.6725 - lr: 0.1000\n",
      "Epoch 2/1000\n",
      "1506/1506 [==============================] - 141s 94ms/step - loss: 0.8698 - accuracy: 0.8112 - val_loss: 0.7059 - val_accuracy: 0.8472 - lr: 0.1000\n",
      "Epoch 3/1000\n",
      "1506/1506 [==============================] - 141s 94ms/step - loss: 0.7134 - accuracy: 0.8483 - val_loss: 0.6132 - val_accuracy: 0.8665 - lr: 0.1000\n",
      "Epoch 4/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.6550 - accuracy: 0.8606 - val_loss: 0.5841 - val_accuracy: 0.8787 - lr: 0.1000\n",
      "Epoch 5/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.6094 - accuracy: 0.8701 - val_loss: 0.5688 - val_accuracy: 0.8765 - lr: 0.1000\n",
      "Epoch 6/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.5770 - accuracy: 0.8765 - val_loss: 0.5603 - val_accuracy: 0.8741 - lr: 0.1000\n",
      "Epoch 7/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.5552 - accuracy: 0.8803 - val_loss: 0.5264 - val_accuracy: 0.8813 - lr: 0.1000\n",
      "Epoch 8/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.5378 - accuracy: 0.8826 - val_loss: 0.5003 - val_accuracy: 0.8904 - lr: 0.1000\n",
      "Epoch 9/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.5195 - accuracy: 0.8862 - val_loss: 0.5247 - val_accuracy: 0.8777 - lr: 0.1000\n",
      "Epoch 10/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.5074 - accuracy: 0.8882 - val_loss: 0.5026 - val_accuracy: 0.8780 - lr: 0.1000\n",
      "Epoch 11/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4948 - accuracy: 0.8904 - val_loss: 0.4851 - val_accuracy: 0.8857 - lr: 0.1000\n",
      "Epoch 12/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4839 - accuracy: 0.8910 - val_loss: 0.4700 - val_accuracy: 0.8902 - lr: 0.1000\n",
      "Epoch 13/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4723 - accuracy: 0.8932 - val_loss: 0.4535 - val_accuracy: 0.8923 - lr: 0.1000\n",
      "Epoch 14/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4635 - accuracy: 0.8951 - val_loss: 0.4647 - val_accuracy: 0.8860 - lr: 0.1000\n",
      "Epoch 15/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4560 - accuracy: 0.8970 - val_loss: 0.4268 - val_accuracy: 0.9021 - lr: 0.1000\n",
      "Epoch 16/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.4497 - accuracy: 0.8967 - val_loss: 0.4436 - val_accuracy: 0.8900 - lr: 0.1000\n",
      "Epoch 17/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4426 - accuracy: 0.8986 - val_loss: 0.4416 - val_accuracy: 0.8911 - lr: 0.1000\n",
      "Epoch 18/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4356 - accuracy: 0.8991 - val_loss: 0.4107 - val_accuracy: 0.9031 - lr: 0.1000\n",
      "Epoch 19/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.4298 - accuracy: 0.9000 - val_loss: 0.4282 - val_accuracy: 0.8912 - lr: 0.1000\n",
      "Epoch 20/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.4261 - accuracy: 0.9006 - val_loss: 0.4431 - val_accuracy: 0.8889 - lr: 0.1000\n",
      "Epoch 21/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.4198 - accuracy: 0.9014 - val_loss: 0.4000 - val_accuracy: 0.9016 - lr: 0.1000\n",
      "Epoch 22/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.4155 - accuracy: 0.9024 - val_loss: 0.4172 - val_accuracy: 0.8939 - lr: 0.1000\n",
      "Epoch 23/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4122 - accuracy: 0.9027 - val_loss: 0.4081 - val_accuracy: 0.8972 - lr: 0.1000\n",
      "Epoch 24/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4059 - accuracy: 0.9037 - val_loss: 0.3996 - val_accuracy: 0.9007 - lr: 0.1000\n",
      "Epoch 25/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.4030 - accuracy: 0.9042 - val_loss: 0.3961 - val_accuracy: 0.9010 - lr: 0.1000\n",
      "Epoch 26/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3994 - accuracy: 0.9047 - val_loss: 0.3876 - val_accuracy: 0.9019 - lr: 0.1000\n",
      "Epoch 27/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3954 - accuracy: 0.9057 - val_loss: 0.4097 - val_accuracy: 0.8913 - lr: 0.1000\n",
      "Epoch 28/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3911 - accuracy: 0.9058 - val_loss: 0.3914 - val_accuracy: 0.8998 - lr: 0.1000\n",
      "Epoch 29/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3890 - accuracy: 0.9057 - val_loss: 0.3937 - val_accuracy: 0.8973 - lr: 0.1000\n",
      "Epoch 30/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3846 - accuracy: 0.9075 - val_loss: 0.3775 - val_accuracy: 0.9060 - lr: 0.1000\n",
      "Epoch 31/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3842 - accuracy: 0.9065 - val_loss: 0.3644 - val_accuracy: 0.9076 - lr: 0.1000\n",
      "Epoch 32/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3813 - accuracy: 0.9073 - val_loss: 0.3713 - val_accuracy: 0.9053 - lr: 0.1000\n",
      "Epoch 33/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3794 - accuracy: 0.9075 - val_loss: 0.3849 - val_accuracy: 0.8997 - lr: 0.1000\n",
      "Epoch 34/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3740 - accuracy: 0.9085 - val_loss: 0.3646 - val_accuracy: 0.9073 - lr: 0.1000\n",
      "Epoch 35/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3697 - accuracy: 0.9104 - val_loss: 0.3576 - val_accuracy: 0.9101 - lr: 0.1000\n",
      "Epoch 36/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3694 - accuracy: 0.9093 - val_loss: 0.3844 - val_accuracy: 0.8978 - lr: 0.1000\n",
      "Epoch 37/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3662 - accuracy: 0.9092 - val_loss: 0.3805 - val_accuracy: 0.8995 - lr: 0.1000\n",
      "Epoch 38/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3650 - accuracy: 0.9109 - val_loss: 0.3571 - val_accuracy: 0.9083 - lr: 0.1000\n",
      "Epoch 39/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3649 - accuracy: 0.9109 - val_loss: 0.3617 - val_accuracy: 0.9078 - lr: 0.1000\n",
      "Epoch 40/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3599 - accuracy: 0.9105 - val_loss: 0.3678 - val_accuracy: 0.9031 - lr: 0.1000\n",
      "Epoch 41/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3595 - accuracy: 0.9112 - val_loss: 0.3610 - val_accuracy: 0.9042 - lr: 0.1000\n",
      "Epoch 42/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3559 - accuracy: 0.9121 - val_loss: 0.3540 - val_accuracy: 0.9072 - lr: 0.1000\n",
      "Epoch 43/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3550 - accuracy: 0.9130 - val_loss: 0.3559 - val_accuracy: 0.9072 - lr: 0.1000\n",
      "Epoch 44/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3521 - accuracy: 0.9136 - val_loss: 0.3676 - val_accuracy: 0.9013 - lr: 0.1000\n",
      "Epoch 45/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3519 - accuracy: 0.9125 - val_loss: 0.3485 - val_accuracy: 0.9065 - lr: 0.1000\n",
      "Epoch 46/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3492 - accuracy: 0.9135 - val_loss: 0.3474 - val_accuracy: 0.9074 - lr: 0.1000\n",
      "Epoch 47/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3484 - accuracy: 0.9133 - val_loss: 0.3439 - val_accuracy: 0.9092 - lr: 0.1000\n",
      "Epoch 48/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3450 - accuracy: 0.9144 - val_loss: 0.3502 - val_accuracy: 0.9061 - lr: 0.1000\n",
      "Epoch 49/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3444 - accuracy: 0.9138 - val_loss: 0.3494 - val_accuracy: 0.9051 - lr: 0.1000\n",
      "Epoch 50/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3431 - accuracy: 0.9141 - val_loss: 0.3436 - val_accuracy: 0.9089 - lr: 0.1000\n",
      "Epoch 51/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3397 - accuracy: 0.9158 - val_loss: 0.3594 - val_accuracy: 0.9014 - lr: 0.1000\n",
      "Epoch 52/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3396 - accuracy: 0.9154 - val_loss: 0.3364 - val_accuracy: 0.9113 - lr: 0.1000\n",
      "Epoch 53/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3391 - accuracy: 0.9147 - val_loss: 0.3318 - val_accuracy: 0.9117 - lr: 0.1000\n",
      "Epoch 54/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3369 - accuracy: 0.9160 - val_loss: 0.3304 - val_accuracy: 0.9141 - lr: 0.1000\n",
      "Epoch 55/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3345 - accuracy: 0.9166 - val_loss: 0.3302 - val_accuracy: 0.9121 - lr: 0.1000\n",
      "Epoch 56/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3339 - accuracy: 0.9166 - val_loss: 0.3408 - val_accuracy: 0.9085 - lr: 0.1000\n",
      "Epoch 57/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3328 - accuracy: 0.9165 - val_loss: 0.3344 - val_accuracy: 0.9105 - lr: 0.1000\n",
      "Epoch 58/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3305 - accuracy: 0.9171 - val_loss: 0.3324 - val_accuracy: 0.9107 - lr: 0.1000\n",
      "Epoch 59/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3306 - accuracy: 0.9169 - val_loss: 0.3259 - val_accuracy: 0.9125 - lr: 0.1000\n",
      "Epoch 60/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3280 - accuracy: 0.9171 - val_loss: 0.3501 - val_accuracy: 0.9037 - lr: 0.1000\n",
      "Epoch 61/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.3276 - accuracy: 0.9172 - val_loss: 0.3458 - val_accuracy: 0.9051 - lr: 0.1000\n",
      "Epoch 62/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3264 - accuracy: 0.9183 - val_loss: 0.3346 - val_accuracy: 0.9099 - lr: 0.1000\n",
      "Epoch 63/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3247 - accuracy: 0.9181 - val_loss: 0.3331 - val_accuracy: 0.9093 - lr: 0.1000\n",
      "Epoch 64/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3237 - accuracy: 0.9182 - val_loss: 0.3327 - val_accuracy: 0.9099 - lr: 0.1000\n",
      "Epoch 65/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3222 - accuracy: 0.9182 - val_loss: 0.3229 - val_accuracy: 0.9138 - lr: 0.1000\n",
      "Epoch 66/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3209 - accuracy: 0.9185 - val_loss: 0.3307 - val_accuracy: 0.9109 - lr: 0.1000\n",
      "Epoch 67/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3195 - accuracy: 0.9189 - val_loss: 0.3315 - val_accuracy: 0.9101 - lr: 0.1000\n",
      "Epoch 68/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3188 - accuracy: 0.9194 - val_loss: 0.3243 - val_accuracy: 0.9124 - lr: 0.1000\n",
      "Epoch 69/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3175 - accuracy: 0.9209 - val_loss: 0.3216 - val_accuracy: 0.9123 - lr: 0.1000\n",
      "Epoch 70/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3168 - accuracy: 0.9200 - val_loss: 0.3080 - val_accuracy: 0.9198 - lr: 0.1000\n",
      "Epoch 71/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3155 - accuracy: 0.9201 - val_loss: 0.3114 - val_accuracy: 0.9168 - lr: 0.1000\n",
      "Epoch 72/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3149 - accuracy: 0.9204 - val_loss: 0.3169 - val_accuracy: 0.9142 - lr: 0.1000\n",
      "Epoch 73/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3151 - accuracy: 0.9198 - val_loss: 0.3057 - val_accuracy: 0.9199 - lr: 0.1000\n",
      "Epoch 74/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3131 - accuracy: 0.9199 - val_loss: 0.3040 - val_accuracy: 0.9207 - lr: 0.1000\n",
      "Epoch 75/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3113 - accuracy: 0.9209 - val_loss: 0.3080 - val_accuracy: 0.9177 - lr: 0.1000\n",
      "Epoch 76/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3089 - accuracy: 0.9215 - val_loss: 0.3146 - val_accuracy: 0.9162 - lr: 0.1000\n",
      "Epoch 77/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3109 - accuracy: 0.9208 - val_loss: 0.3064 - val_accuracy: 0.9178 - lr: 0.1000\n",
      "Epoch 78/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.3077 - accuracy: 0.9219 - val_loss: 0.3049 - val_accuracy: 0.9182 - lr: 0.1000\n",
      "Epoch 79/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3078 - accuracy: 0.9220 - val_loss: 0.3098 - val_accuracy: 0.9159 - lr: 0.1000\n",
      "Epoch 80/1000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.9214\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 0.020000000298023225.\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.3079 - accuracy: 0.9214 - val_loss: 0.3086 - val_accuracy: 0.9161 - lr: 0.1000\n",
      "Epoch 81/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2965 - accuracy: 0.9257 - val_loss: 0.3014 - val_accuracy: 0.9203 - lr: 0.0200\n",
      "Epoch 82/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2945 - accuracy: 0.9269 - val_loss: 0.2981 - val_accuracy: 0.9214 - lr: 0.0200\n",
      "Epoch 83/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2922 - accuracy: 0.9285 - val_loss: 0.2948 - val_accuracy: 0.9230 - lr: 0.0200\n",
      "Epoch 84/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2922 - accuracy: 0.9273 - val_loss: 0.2925 - val_accuracy: 0.9229 - lr: 0.0200\n",
      "Epoch 85/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2910 - accuracy: 0.9285 - val_loss: 0.2982 - val_accuracy: 0.9207 - lr: 0.0200\n",
      "Epoch 86/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2900 - accuracy: 0.9281 - val_loss: 0.2951 - val_accuracy: 0.9223 - lr: 0.0200\n",
      "Epoch 87/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2907 - accuracy: 0.9282 - val_loss: 0.2961 - val_accuracy: 0.9219 - lr: 0.0200\n",
      "Epoch 88/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2895 - accuracy: 0.9288 - val_loss: 0.2974 - val_accuracy: 0.9209 - lr: 0.0200\n",
      "Epoch 89/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2890 - accuracy: 0.9290 - val_loss: 0.2934 - val_accuracy: 0.9227 - lr: 0.0200\n",
      "Epoch 90/1000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9281\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.003999999910593033.\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2903 - accuracy: 0.9281 - val_loss: 0.2963 - val_accuracy: 0.9219 - lr: 0.0200\n",
      "Epoch 91/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2873 - accuracy: 0.9291 - val_loss: 0.2901 - val_accuracy: 0.9243 - lr: 0.0040\n",
      "Epoch 92/1000\n",
      "1506/1506 [==============================] - 141s 94ms/step - loss: 0.2884 - accuracy: 0.9293 - val_loss: 0.2942 - val_accuracy: 0.9234 - lr: 0.0040\n",
      "Epoch 93/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2862 - accuracy: 0.9305 - val_loss: 0.2971 - val_accuracy: 0.9226 - lr: 0.0040\n",
      "Epoch 94/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2880 - accuracy: 0.9295 - val_loss: 0.2926 - val_accuracy: 0.9233 - lr: 0.0040\n",
      "Epoch 95/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2866 - accuracy: 0.9299 - val_loss: 0.2949 - val_accuracy: 0.9226 - lr: 0.0040\n",
      "Epoch 96/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2858 - accuracy: 0.9308 - val_loss: 0.2936 - val_accuracy: 0.9230 - lr: 0.0040\n",
      "Epoch 97/1000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9303\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.0007999999448657036.\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2872 - accuracy: 0.9303 - val_loss: 0.2955 - val_accuracy: 0.9225 - lr: 0.0040\n",
      "Epoch 98/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2850 - accuracy: 0.9313 - val_loss: 0.2908 - val_accuracy: 0.9244 - lr: 8.0000e-04\n",
      "Epoch 99/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2866 - accuracy: 0.9299 - val_loss: 0.2949 - val_accuracy: 0.9229 - lr: 8.0000e-04\n",
      "Epoch 100/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2853 - accuracy: 0.9308 - val_loss: 0.2935 - val_accuracy: 0.9235 - lr: 8.0000e-04\n",
      "Epoch 101/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2868 - accuracy: 0.9299 - val_loss: 0.2962 - val_accuracy: 0.9220 - lr: 8.0000e-04\n",
      "Epoch 102/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2854 - accuracy: 0.9308 - val_loss: 0.2924 - val_accuracy: 0.9235 - lr: 8.0000e-04\n",
      "Epoch 103/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2852 - accuracy: 0.9310 - val_loss: 0.2893 - val_accuracy: 0.9246 - lr: 8.0000e-04\n",
      "Epoch 104/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2868 - accuracy: 0.9293 - val_loss: 0.2904 - val_accuracy: 0.9242 - lr: 8.0000e-04\n",
      "Epoch 105/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2855 - accuracy: 0.9311 - val_loss: 0.2936 - val_accuracy: 0.9234 - lr: 8.0000e-04\n",
      "Epoch 106/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2845 - accuracy: 0.9310 - val_loss: 0.2940 - val_accuracy: 0.9228 - lr: 8.0000e-04\n",
      "Epoch 107/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2841 - accuracy: 0.9319 - val_loss: 0.2925 - val_accuracy: 0.9244 - lr: 8.0000e-04\n",
      "Epoch 108/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2855 - accuracy: 0.9305 - val_loss: 0.2943 - val_accuracy: 0.9233 - lr: 8.0000e-04\n",
      "Epoch 109/1000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.9305\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 0.00015999998431652786.\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2858 - accuracy: 0.9305 - val_loss: 0.2918 - val_accuracy: 0.9237 - lr: 8.0000e-04\n",
      "Epoch 110/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2858 - accuracy: 0.9300 - val_loss: 0.2915 - val_accuracy: 0.9242 - lr: 1.6000e-04\n",
      "Epoch 111/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2841 - accuracy: 0.9318 - val_loss: 0.2884 - val_accuracy: 0.9249 - lr: 1.6000e-04\n",
      "Epoch 112/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2866 - accuracy: 0.9301 - val_loss: 0.2937 - val_accuracy: 0.9234 - lr: 1.6000e-04\n",
      "Epoch 113/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2847 - accuracy: 0.9314 - val_loss: 0.2941 - val_accuracy: 0.9230 - lr: 1.6000e-04\n",
      "Epoch 114/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2861 - accuracy: 0.9305 - val_loss: 0.2892 - val_accuracy: 0.9242 - lr: 1.6000e-04\n",
      "Epoch 115/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2850 - accuracy: 0.9305 - val_loss: 0.2912 - val_accuracy: 0.9244 - lr: 1.6000e-04\n",
      "Epoch 116/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2864 - accuracy: 0.9302 - val_loss: 0.2918 - val_accuracy: 0.9241 - lr: 1.6000e-04\n",
      "Epoch 117/1000\n",
      "1506/1506 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.9303\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2847 - accuracy: 0.9303 - val_loss: 0.2924 - val_accuracy: 0.9239 - lr: 1.6000e-04\n",
      "Epoch 118/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2864 - accuracy: 0.9296 - val_loss: 0.2909 - val_accuracy: 0.9245 - lr: 1.0000e-04\n",
      "Epoch 119/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2846 - accuracy: 0.9320 - val_loss: 0.2938 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 120/1000\n",
      "1506/1506 [==============================] - 142s 95ms/step - loss: 0.2841 - accuracy: 0.9316 - val_loss: 0.2911 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Epoch 121/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2858 - accuracy: 0.9302 - val_loss: 0.2920 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 122/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2863 - accuracy: 0.9297 - val_loss: 0.2901 - val_accuracy: 0.9243 - lr: 1.0000e-04\n",
      "Epoch 123/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2856 - accuracy: 0.9304 - val_loss: 0.2937 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 124/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2861 - accuracy: 0.9297 - val_loss: 0.2943 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Epoch 125/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2862 - accuracy: 0.9291 - val_loss: 0.2933 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Epoch 126/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2860 - accuracy: 0.9304 - val_loss: 0.2898 - val_accuracy: 0.9247 - lr: 1.0000e-04\n",
      "Epoch 127/1000\n",
      "1506/1506 [==============================] - 143s 95ms/step - loss: 0.2859 - accuracy: 0.9306 - val_loss: 0.2910 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Epoch 128/1000\n",
      "1506/1506 [==============================] - 141s 93ms/step - loss: 0.2860 - accuracy: 0.9306 - val_loss: 0.2906 - val_accuracy: 0.9239 - lr: 1.0000e-04\n",
      "Epoch 129/1000\n",
      "1506/1506 [==============================] - 142s 94ms/step - loss: 0.2862 - accuracy: 0.9308 - val_loss: 0.2902 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Epoch 129: early stopping\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=classWeight,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Графіки тренування моделі\n",
    "plt.figure()\n",
    "plt.plot(H.history['loss'], label='train_loss')\n",
    "plt.plot(H.history['val_loss'], label='val_loss')\n",
    "plt.plot(H.history['accuracy'], label='train_acc')\n",
    "plt.plot(H.history['val_accuracy'], label='val_acc')\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Ukrainian_OCR_extended_Resnet_with_blure_and_aug_new.h5',save_format=\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('Ukrainian_OCR_extended_Resnet_with_blure_and_aug_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# evaluate the network\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[INFO] evaluating network...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(testX, batch_size\u001b[39m=\u001b[39;49mBS)\n\u001b[0;32m     10\u001b[0m \u001b[39mprint\u001b[39m(classification_report(testY\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     11\u001b[0m \tpredictions\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), target_names\u001b[39m=\u001b[39mlabelNames))\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# define the list of label names\n",
    "\n",
    "labelNames = \"АБВГҐДЕЄЖЗИІЇЙКЛМНОПРСТУФХЦЧШЩЬЮЯабвгґдеєжзиіїйклмнопрстуфхцчшщьюя1234567890№%@,.?:;\\\"!()-'\"\n",
    "labelNames = [l for l in labelNames]\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "from imutils import build_montages\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "# randomly select a few testing characters\n",
    "\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(66,)):\n",
    "    probs = model.predict(testX[np.newaxis, i])\n",
    "    prediction = probs.argmax(axis=1)\n",
    "    label = labelNames[prediction[0]]\n",
    "    output += label\n",
    "    image = (testX[i] * 255).astype(\"uint8\")\n",
    "    color = (0, 255, 0)\n",
    "    color = (0, 0, 255)\n",
    "    image = cv2.merge([image] * 3)\n",
    "    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "    img_pil = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=34)\n",
    "    draw.text((5, 20), label, font=font, fill=color)\n",
    "    image = np.array(img_pil)\n",
    "    images.append(image)\n",
    "  \n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "cv2.imshow('q',montage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'гЛЄРНезцХВЙжЮЕйткЗПццЗнТчХЧСрОРаФЬЩҐтИЖйИАОтЯмрЛЩ'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
