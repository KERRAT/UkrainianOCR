{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset(datasetPath):\n",
    "\n",
    "  # List for storing data\n",
    "  data = []\n",
    "  \n",
    "  # List for storing labels\n",
    "  labels = []\n",
    "  \n",
    "  for row in open(datasetPath): #Openfile and start reading each row\n",
    "    #Split the row at every comma\n",
    "    row = row.split(\",\")\n",
    "    \n",
    "    #row[0] contains label\n",
    "    label = int(row[0])\n",
    "    \n",
    "    #Other all collumns contains pixel values make a saperate array for that\n",
    "    image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "    \n",
    "    #Reshaping image to 28 x 28 pixels\n",
    "    image = image.reshape((28, 28))\n",
    "    \n",
    "    #append image to data\n",
    "    data.append(image)\n",
    "    \n",
    "    #append label to labels\n",
    "    labels.append(label)\n",
    "    \n",
    "  #Converting data to numpy array of type float32\n",
    "  data = np.array(data, dtype='float32')\n",
    "  \n",
    "  #Converting labels to type int\n",
    "  labels = np.array(labels, dtype=\"int\")\n",
    "  \n",
    "  return (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Data, Labels) = load_dataset(\"../data/uaset_without_columns.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Data, Labels) = load_dataset(\"../data/uaset_extended_with_digits_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# However, the architecture we're using is designed for 32x32 images,\n",
    "# So we need to resize them to 32x32\n",
    "\n",
    "Data = [cv2.resize(image, (32, 32)) for image in Data]\n",
    "Data = np.array(Data, dtype=\"float32\")\n",
    "\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "\n",
    "Data = np.expand_dims(Data, axis=-1)\n",
    "Data /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "le = LabelBinarizer()\n",
    "Labels = le.fit_transform(Labels)\n",
    "\n",
    "counts = Labels.sum(axis=0)\n",
    "\n",
    "# account for skew in the labeled data\n",
    "classTotals = Labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "  classWeight[i] = classTotals.max() / classTotals[i]\n",
    "  \n",
    "(trainX, testX, trainY, testY) = train_test_split(Data,\n",
    "\tLabels, test_size=0.95, stratify=Labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "rotation_range=10,\n",
    "zoom_range=0.04,\n",
    "width_shift_range=0.15,\n",
    "height_shift_range=0.15,\n",
    "shear_range=0.2,\n",
    "horizontal_flip=False,\n",
    "fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import add\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and apply BN\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\n",
    "\t\t# check if we are utilizing the CIFAR dataset\n",
    "\t\tif dataset == \"cifar\":\n",
    "\t\t\t# apply a single CONV layer\n",
    "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
    "\t\telif dataset == \"tiny_imagenet\":\n",
    "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\t\tmomentum=bnMom)(x)\n",
    "\t\t\tx = Activation(\"relu\")(x)\n",
    "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
    "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\t\t# loop over the number of stages\n",
    "\t\tfor i in range(0, len(stages)):\n",
    "\t\t\t# initialize the stride, then apply a residual module\n",
    "\t\t\t# used to reduce the spatial size of the input volume\n",
    "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t\t# loop over the number of layers in the stage\n",
    "\t\t\tfor j in range(0, stages[i] - 1):\n",
    "\t\t\t\t# apply a ResNet module\n",
    "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "INIT_LR = 1e-2\n",
    "BS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 32, 32, 1)   4           ['input_3[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 32, 32, 64)   576         ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 32, 32, 64)  256         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 32, 32, 16)   1024        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 32, 32, 16)  64          ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 32, 32, 16)   2304        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 32, 32, 16)  64          ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 32, 32, 64)   1024        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 32, 32, 64)   4096        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 32, 32, 64)   0           ['conv2d_65[0][0]',              \n",
      "                                                                  'conv2d_66[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 32, 32, 64)  256         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 32, 32, 16)   1024        ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 32, 32, 16)  64          ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 32, 32, 16)   2304        ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 32, 32, 16)  64          ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 32, 32, 64)   1024        ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 32, 32, 64)   0           ['conv2d_69[0][0]',              \n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 32, 32, 64)  256         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 32, 32, 16)   1024        ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 32, 32, 16)  64          ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 32, 32, 16)   2304        ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 32, 32, 16)  64          ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 32, 32, 64)   1024        ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 32, 32, 64)   0           ['conv2d_72[0][0]',              \n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 32, 32, 64)  256         ['add_20[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 32, 32, 64)   0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 32, 32, 32)   2048        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 32, 32, 32)  128         ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 32)   9216        ['activation_68[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 16, 16, 32)  128         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 16, 16, 128)  4096        ['activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 16, 16, 128)  8192        ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 16, 16, 128)  0           ['conv2d_75[0][0]',              \n",
      "                                                                  'conv2d_76[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 16, 16, 128)  512        ['add_21[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 16, 16, 32)   4096        ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 16, 16, 32)  128         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 16, 16, 32)   9216        ['activation_71[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 16, 16, 32)  128         ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 16, 16, 128)  4096        ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 16, 16, 128)  0           ['conv2d_79[0][0]',              \n",
      "                                                                  'add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 16, 16, 128)  512        ['add_22[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 16, 16, 32)   4096        ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 16, 16, 32)  128         ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 16, 16, 32)   9216        ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 16, 16, 32)  128         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 16, 16, 128)  4096        ['activation_75[0][0]']          \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 16, 16, 128)  0           ['conv2d_82[0][0]',              \n",
      "                                                                  'add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 16, 16, 128)  512        ['add_23[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 16, 16, 128)  0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 16, 16, 64)   8192        ['activation_76[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 16, 16, 64)  256         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 8, 8, 64)     36864       ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 8, 8, 64)    256         ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 8, 8, 256)    16384       ['activation_78[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 8, 8, 256)    32768       ['activation_76[0][0]']          \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 8, 8, 256)    0           ['conv2d_85[0][0]',              \n",
      "                                                                  'conv2d_86[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 8, 8, 256)   1024        ['add_24[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 8, 8, 64)     16384       ['activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 8, 8, 64)    256         ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 8, 8, 64)     36864       ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 8, 8, 64)    256         ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 8, 8, 256)    16384       ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 8, 8, 256)    0           ['conv2d_89[0][0]',              \n",
      "                                                                  'add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 8, 8, 256)   1024        ['add_25[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 8, 8, 64)     16384       ['activation_82[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 8, 8, 64)    256         ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 8, 8, 64)     36864       ['activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 8, 8, 64)    256         ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 8, 8, 256)    16384       ['activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 8, 8, 256)    0           ['conv2d_92[0][0]',              \n",
      "                                                                  'add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 8, 8, 256)   1024        ['add_26[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 8, 8, 256)    0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 1, 1, 256)   0           ['activation_85[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 256)          0           ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 90)           23130       ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 90)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 341,022\n",
      "Trainable params: 336,860\n",
      "Non-trainable params: 4,162\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "139/139 [==============================] - 18s 104ms/step - loss: 4.7572 - accuracy: 0.0171 - val_loss: 4.7127 - val_accuracy: 0.0187\n",
      "Epoch 2/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 4.6719 - accuracy: 0.0278 - val_loss: 4.6331 - val_accuracy: 0.0239\n",
      "Epoch 3/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 4.6168 - accuracy: 0.0351 - val_loss: 4.5674 - val_accuracy: 0.0361\n",
      "Epoch 4/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 4.5638 - accuracy: 0.0422 - val_loss: 4.5077 - val_accuracy: 0.0419\n",
      "Epoch 5/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 4.5066 - accuracy: 0.0508 - val_loss: 4.4352 - val_accuracy: 0.0446\n",
      "Epoch 6/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 4.4395 - accuracy: 0.0567 - val_loss: 4.3622 - val_accuracy: 0.0479\n",
      "Epoch 7/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 4.3612 - accuracy: 0.0638 - val_loss: 4.2695 - val_accuracy: 0.0664\n",
      "Epoch 8/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 4.2825 - accuracy: 0.0765 - val_loss: 4.1820 - val_accuracy: 0.0707\n",
      "Epoch 9/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 4.2013 - accuracy: 0.0849 - val_loss: 4.0975 - val_accuracy: 0.0749\n",
      "Epoch 10/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 4.1187 - accuracy: 0.1092 - val_loss: 4.0237 - val_accuracy: 0.0797\n",
      "Epoch 11/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 4.0341 - accuracy: 0.1248 - val_loss: 3.9557 - val_accuracy: 0.1010\n",
      "Epoch 12/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 3.9480 - accuracy: 0.1339 - val_loss: 3.8173 - val_accuracy: 0.1297\n",
      "Epoch 13/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 3.8590 - accuracy: 0.1551 - val_loss: 3.7698 - val_accuracy: 0.1548\n",
      "Epoch 14/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 3.7718 - accuracy: 0.1670 - val_loss: 3.6629 - val_accuracy: 0.1637\n",
      "Epoch 15/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 3.6858 - accuracy: 0.1859 - val_loss: 3.5822 - val_accuracy: 0.2054\n",
      "Epoch 16/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 3.6030 - accuracy: 0.2095 - val_loss: 3.4988 - val_accuracy: 0.2108\n",
      "Epoch 17/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 3.5176 - accuracy: 0.2250 - val_loss: 3.5191 - val_accuracy: 0.2021\n",
      "Epoch 18/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 3.4329 - accuracy: 0.2667 - val_loss: 3.3185 - val_accuracy: 0.2454\n",
      "Epoch 19/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 3.3529 - accuracy: 0.2857 - val_loss: 3.2806 - val_accuracy: 0.2907\n",
      "Epoch 20/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 3.2720 - accuracy: 0.3127 - val_loss: 3.1649 - val_accuracy: 0.3172\n",
      "Epoch 21/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 3.1952 - accuracy: 0.3395 - val_loss: 3.1175 - val_accuracy: 0.3537\n",
      "Epoch 22/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 3.1155 - accuracy: 0.3567 - val_loss: 3.0683 - val_accuracy: 0.3566\n",
      "Epoch 23/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 3.0451 - accuracy: 0.3791 - val_loss: 2.9036 - val_accuracy: 0.4160\n",
      "Epoch 24/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 2.9556 - accuracy: 0.4122 - val_loss: 2.8310 - val_accuracy: 0.4087\n",
      "Epoch 25/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 2.8888 - accuracy: 0.4214 - val_loss: 2.8036 - val_accuracy: 0.4243\n",
      "Epoch 26/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 2.8089 - accuracy: 0.4554 - val_loss: 2.7065 - val_accuracy: 0.4475\n",
      "Epoch 27/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 2.7335 - accuracy: 0.4827 - val_loss: 2.6533 - val_accuracy: 0.5062\n",
      "Epoch 28/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 2.6630 - accuracy: 0.4917 - val_loss: 2.5521 - val_accuracy: 0.5301\n",
      "Epoch 29/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 2.5916 - accuracy: 0.5129 - val_loss: 2.4516 - val_accuracy: 0.5239\n",
      "Epoch 30/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 2.5143 - accuracy: 0.5296 - val_loss: 2.3775 - val_accuracy: 0.5485\n",
      "Epoch 31/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 2.4445 - accuracy: 0.5526 - val_loss: 2.3191 - val_accuracy: 0.5988\n",
      "Epoch 32/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 2.3754 - accuracy: 0.5780 - val_loss: 2.4125 - val_accuracy: 0.4629\n",
      "Epoch 33/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 2.3013 - accuracy: 0.5894 - val_loss: 2.1103 - val_accuracy: 0.6207\n",
      "Epoch 34/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 2.2421 - accuracy: 0.5987 - val_loss: 2.0653 - val_accuracy: 0.6274\n",
      "Epoch 35/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 2.1763 - accuracy: 0.6203 - val_loss: 1.9648 - val_accuracy: 0.6714\n",
      "Epoch 36/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 2.1045 - accuracy: 0.6353 - val_loss: 1.9930 - val_accuracy: 0.6492\n",
      "Epoch 37/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 2.0481 - accuracy: 0.6465 - val_loss: 1.9872 - val_accuracy: 0.6257\n",
      "Epoch 38/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.9754 - accuracy: 0.6616 - val_loss: 1.7897 - val_accuracy: 0.7135\n",
      "Epoch 39/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.9139 - accuracy: 0.6769 - val_loss: 1.7860 - val_accuracy: 0.7147\n",
      "Epoch 40/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.8459 - accuracy: 0.6865 - val_loss: 1.6614 - val_accuracy: 0.7359\n",
      "Epoch 41/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.8049 - accuracy: 0.6904 - val_loss: 1.7712 - val_accuracy: 0.6413\n",
      "Epoch 42/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.7621 - accuracy: 0.6921 - val_loss: 1.5698 - val_accuracy: 0.7519\n",
      "Epoch 43/1000\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 1.6961 - accuracy: 0.7076 - val_loss: 1.5516 - val_accuracy: 0.7541\n",
      "Epoch 44/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.6423 - accuracy: 0.7135 - val_loss: 1.4751 - val_accuracy: 0.7587\n",
      "Epoch 45/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.6063 - accuracy: 0.7242 - val_loss: 1.5246 - val_accuracy: 0.7299\n",
      "Epoch 46/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 1.5582 - accuracy: 0.7367 - val_loss: 1.3984 - val_accuracy: 0.7585\n",
      "Epoch 47/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.5101 - accuracy: 0.7409 - val_loss: 1.4167 - val_accuracy: 0.7485\n",
      "Epoch 48/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.4767 - accuracy: 0.7447 - val_loss: 1.5591 - val_accuracy: 0.6940\n",
      "Epoch 49/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.4276 - accuracy: 0.7568 - val_loss: 1.2698 - val_accuracy: 0.7890\n",
      "Epoch 50/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.3966 - accuracy: 0.7645 - val_loss: 1.3284 - val_accuracy: 0.7629\n",
      "Epoch 51/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.3698 - accuracy: 0.7606 - val_loss: 1.2156 - val_accuracy: 0.7925\n",
      "Epoch 52/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.3247 - accuracy: 0.7729 - val_loss: 1.2382 - val_accuracy: 0.7753\n",
      "Epoch 53/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.2947 - accuracy: 0.7813 - val_loss: 1.2324 - val_accuracy: 0.7849\n",
      "Epoch 54/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.2638 - accuracy: 0.7828 - val_loss: 1.1363 - val_accuracy: 0.8199\n",
      "Epoch 55/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.2381 - accuracy: 0.7900 - val_loss: 1.0827 - val_accuracy: 0.8191\n",
      "Epoch 56/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 1.2025 - accuracy: 0.7966 - val_loss: 1.0796 - val_accuracy: 0.8218\n",
      "Epoch 57/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.1769 - accuracy: 0.8040 - val_loss: 1.0564 - val_accuracy: 0.8249\n",
      "Epoch 58/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.1541 - accuracy: 0.8016 - val_loss: 1.0133 - val_accuracy: 0.8479\n",
      "Epoch 59/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.1278 - accuracy: 0.8170 - val_loss: 1.0268 - val_accuracy: 0.8363\n",
      "Epoch 60/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.1045 - accuracy: 0.8167 - val_loss: 1.1173 - val_accuracy: 0.7965\n",
      "Epoch 61/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.0898 - accuracy: 0.8169 - val_loss: 1.0414 - val_accuracy: 0.7988\n",
      "Epoch 62/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.0627 - accuracy: 0.8251 - val_loss: 0.9340 - val_accuracy: 0.8564\n",
      "Epoch 63/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.0511 - accuracy: 0.8226 - val_loss: 1.0386 - val_accuracy: 0.7998\n",
      "Epoch 64/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 1.0169 - accuracy: 0.8349 - val_loss: 0.9335 - val_accuracy: 0.8419\n",
      "Epoch 65/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 1.0026 - accuracy: 0.8368 - val_loss: 0.9056 - val_accuracy: 0.8467\n",
      "Epoch 66/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.9804 - accuracy: 0.8422 - val_loss: 1.0148 - val_accuracy: 0.8029\n",
      "Epoch 67/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.9718 - accuracy: 0.8454 - val_loss: 0.8747 - val_accuracy: 0.8577\n",
      "Epoch 68/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.9491 - accuracy: 0.8444 - val_loss: 1.0280 - val_accuracy: 0.7954\n",
      "Epoch 69/1000\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 0.9394 - accuracy: 0.8476 - val_loss: 0.9430 - val_accuracy: 0.8058\n",
      "Epoch 70/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.9257 - accuracy: 0.8528 - val_loss: 0.9705 - val_accuracy: 0.8141\n",
      "Epoch 71/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.9153 - accuracy: 0.8552 - val_loss: 1.2863 - val_accuracy: 0.7541\n",
      "Epoch 72/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.8975 - accuracy: 0.8591 - val_loss: 0.8206 - val_accuracy: 0.8645\n",
      "Epoch 73/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.8833 - accuracy: 0.8605 - val_loss: 0.7866 - val_accuracy: 0.8801\n",
      "Epoch 74/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.8757 - accuracy: 0.8638 - val_loss: 0.7790 - val_accuracy: 0.8774\n",
      "Epoch 75/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.8601 - accuracy: 0.8692 - val_loss: 0.8230 - val_accuracy: 0.8743\n",
      "Epoch 76/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.8509 - accuracy: 0.8599 - val_loss: 0.8001 - val_accuracy: 0.8701\n",
      "Epoch 77/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.8370 - accuracy: 0.8707 - val_loss: 0.8080 - val_accuracy: 0.8722\n",
      "Epoch 78/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.8287 - accuracy: 0.8735 - val_loss: 0.7347 - val_accuracy: 0.8882\n",
      "Epoch 79/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.8237 - accuracy: 0.8717 - val_loss: 0.7716 - val_accuracy: 0.8780\n",
      "Epoch 80/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.8101 - accuracy: 0.8762 - val_loss: 0.7614 - val_accuracy: 0.8832\n",
      "Epoch 81/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.8031 - accuracy: 0.8744 - val_loss: 0.7203 - val_accuracy: 0.8896\n",
      "Epoch 82/1000\n",
      "139/139 [==============================] - 15s 105ms/step - loss: 0.7963 - accuracy: 0.8800 - val_loss: 0.7422 - val_accuracy: 0.8772\n",
      "Epoch 83/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7815 - accuracy: 0.8817 - val_loss: 0.7444 - val_accuracy: 0.8749\n",
      "Epoch 84/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.7767 - accuracy: 0.8786 - val_loss: 0.7209 - val_accuracy: 0.8878\n",
      "Epoch 85/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7730 - accuracy: 0.8789 - val_loss: 0.7558 - val_accuracy: 0.8666\n",
      "Epoch 86/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7610 - accuracy: 0.8834 - val_loss: 0.6696 - val_accuracy: 0.9048\n",
      "Epoch 87/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7657 - accuracy: 0.8770 - val_loss: 0.6940 - val_accuracy: 0.8917\n",
      "Epoch 88/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7470 - accuracy: 0.8849 - val_loss: 0.7113 - val_accuracy: 0.8851\n",
      "Epoch 89/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7421 - accuracy: 0.8849 - val_loss: 0.7218 - val_accuracy: 0.8747\n",
      "Epoch 90/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.7364 - accuracy: 0.8872 - val_loss: 0.6725 - val_accuracy: 0.8979\n",
      "Epoch 91/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7224 - accuracy: 0.8922 - val_loss: 0.7613 - val_accuracy: 0.8716\n",
      "Epoch 92/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.7230 - accuracy: 0.8939 - val_loss: 0.6951 - val_accuracy: 0.8795\n",
      "Epoch 93/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.7237 - accuracy: 0.8913 - val_loss: 0.6704 - val_accuracy: 0.8965\n",
      "Epoch 94/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.7182 - accuracy: 0.8955 - val_loss: 0.6617 - val_accuracy: 0.8938\n",
      "Epoch 95/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.7067 - accuracy: 0.8955 - val_loss: 0.6510 - val_accuracy: 0.8975\n",
      "Epoch 96/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.7013 - accuracy: 0.8975 - val_loss: 0.6503 - val_accuracy: 0.8983\n",
      "Epoch 97/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6970 - accuracy: 0.8956 - val_loss: 0.6748 - val_accuracy: 0.8929\n",
      "Epoch 98/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.6957 - accuracy: 0.8964 - val_loss: 0.6606 - val_accuracy: 0.8965\n",
      "Epoch 99/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6922 - accuracy: 0.8958 - val_loss: 0.6636 - val_accuracy: 0.8892\n",
      "Epoch 100/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6865 - accuracy: 0.8989 - val_loss: 0.6614 - val_accuracy: 0.8886\n",
      "Epoch 101/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6709 - accuracy: 0.9032 - val_loss: 0.7155 - val_accuracy: 0.8770\n",
      "Epoch 102/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6734 - accuracy: 0.8991 - val_loss: 0.6830 - val_accuracy: 0.8790\n",
      "Epoch 103/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.6727 - accuracy: 0.8980 - val_loss: 0.6299 - val_accuracy: 0.9023\n",
      "Epoch 104/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6561 - accuracy: 0.9054 - val_loss: 0.6870 - val_accuracy: 0.8886\n",
      "Epoch 105/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6590 - accuracy: 0.9006 - val_loss: 0.6582 - val_accuracy: 0.8934\n",
      "Epoch 106/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.6570 - accuracy: 0.9049 - val_loss: 0.6143 - val_accuracy: 0.9056\n",
      "Epoch 107/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6564 - accuracy: 0.9000 - val_loss: 0.6544 - val_accuracy: 0.8927\n",
      "Epoch 108/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6589 - accuracy: 0.9013 - val_loss: 0.6040 - val_accuracy: 0.9066\n",
      "Epoch 109/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.6573 - accuracy: 0.9005 - val_loss: 0.6157 - val_accuracy: 0.8979\n",
      "Epoch 110/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6425 - accuracy: 0.9065 - val_loss: 0.5995 - val_accuracy: 0.9089\n",
      "Epoch 111/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.6485 - accuracy: 0.9013 - val_loss: 0.6235 - val_accuracy: 0.8959\n",
      "Epoch 112/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6460 - accuracy: 0.9010 - val_loss: 0.6061 - val_accuracy: 0.9041\n",
      "Epoch 113/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6389 - accuracy: 0.9066 - val_loss: 0.6351 - val_accuracy: 0.8975\n",
      "Epoch 114/1000\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 0.6361 - accuracy: 0.9046 - val_loss: 0.6169 - val_accuracy: 0.8956\n",
      "Epoch 115/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.6326 - accuracy: 0.9077 - val_loss: 0.5948 - val_accuracy: 0.9087\n",
      "Epoch 116/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.6235 - accuracy: 0.9051 - val_loss: 0.6008 - val_accuracy: 0.9052\n",
      "Epoch 117/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.6214 - accuracy: 0.9115 - val_loss: 0.6151 - val_accuracy: 0.9044\n",
      "Epoch 118/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6250 - accuracy: 0.9066 - val_loss: 0.6022 - val_accuracy: 0.9021\n",
      "Epoch 119/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6225 - accuracy: 0.9100 - val_loss: 0.6002 - val_accuracy: 0.9050\n",
      "Epoch 120/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.6245 - accuracy: 0.9042 - val_loss: 0.6215 - val_accuracy: 0.8907\n",
      "Epoch 121/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.6138 - accuracy: 0.9104 - val_loss: 0.5905 - val_accuracy: 0.9060\n",
      "Epoch 122/1000\n",
      "139/139 [==============================] - 14s 104ms/step - loss: 0.6017 - accuracy: 0.9137 - val_loss: 0.6453 - val_accuracy: 0.8898\n",
      "Epoch 123/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.6057 - accuracy: 0.9154 - val_loss: 0.5811 - val_accuracy: 0.9104\n",
      "Epoch 124/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.6015 - accuracy: 0.9150 - val_loss: 0.6316 - val_accuracy: 0.8944\n",
      "Epoch 125/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.6143 - accuracy: 0.9068 - val_loss: 0.5986 - val_accuracy: 0.9019\n",
      "Epoch 126/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.5938 - accuracy: 0.9145 - val_loss: 0.6223 - val_accuracy: 0.8929\n",
      "Epoch 127/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.6031 - accuracy: 0.9115 - val_loss: 0.6260 - val_accuracy: 0.8973\n",
      "Epoch 128/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5951 - accuracy: 0.9122 - val_loss: 0.5835 - val_accuracy: 0.9081\n",
      "Epoch 129/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.5925 - accuracy: 0.9145 - val_loss: 0.5643 - val_accuracy: 0.9174\n",
      "Epoch 130/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.5943 - accuracy: 0.9115 - val_loss: 0.6064 - val_accuracy: 0.9008\n",
      "Epoch 131/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.5872 - accuracy: 0.9185 - val_loss: 0.6030 - val_accuracy: 0.8996\n",
      "Epoch 132/1000\n",
      "139/139 [==============================] - 15s 105ms/step - loss: 0.5908 - accuracy: 0.9148 - val_loss: 0.5848 - val_accuracy: 0.9035\n",
      "Epoch 133/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.5868 - accuracy: 0.9156 - val_loss: 0.5803 - val_accuracy: 0.9068\n",
      "Epoch 134/1000\n",
      "139/139 [==============================] - 15s 105ms/step - loss: 0.5830 - accuracy: 0.9164 - val_loss: 0.5631 - val_accuracy: 0.9116\n",
      "Epoch 135/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5923 - accuracy: 0.9142 - val_loss: 0.5859 - val_accuracy: 0.9029\n",
      "Epoch 136/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.5786 - accuracy: 0.9157 - val_loss: 0.5950 - val_accuracy: 0.8934\n",
      "Epoch 137/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.5819 - accuracy: 0.9136 - val_loss: 0.5942 - val_accuracy: 0.8981\n",
      "Epoch 138/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5759 - accuracy: 0.9163 - val_loss: 0.5897 - val_accuracy: 0.9035\n",
      "Epoch 139/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.5734 - accuracy: 0.9193 - val_loss: 0.5731 - val_accuracy: 0.9089\n",
      "Epoch 140/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.5688 - accuracy: 0.9194 - val_loss: 0.5944 - val_accuracy: 0.9017\n",
      "Epoch 141/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.5740 - accuracy: 0.9195 - val_loss: 0.5683 - val_accuracy: 0.9052\n",
      "Epoch 142/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.5726 - accuracy: 0.9196 - val_loss: 0.5716 - val_accuracy: 0.9091\n",
      "Epoch 143/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.5694 - accuracy: 0.9174 - val_loss: 0.5661 - val_accuracy: 0.9093\n",
      "Epoch 144/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.5694 - accuracy: 0.9168 - val_loss: 0.5483 - val_accuracy: 0.9122\n",
      "Epoch 145/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.5618 - accuracy: 0.9190 - val_loss: 0.5643 - val_accuracy: 0.9089\n",
      "Epoch 146/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5641 - accuracy: 0.9160 - val_loss: 0.5536 - val_accuracy: 0.9137\n",
      "Epoch 147/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5674 - accuracy: 0.9171 - val_loss: 0.5535 - val_accuracy: 0.9122\n",
      "Epoch 148/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.5579 - accuracy: 0.9220 - val_loss: 0.5731 - val_accuracy: 0.9091\n",
      "Epoch 149/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5600 - accuracy: 0.9217 - val_loss: 0.5882 - val_accuracy: 0.9054\n",
      "Epoch 150/1000\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 0.5514 - accuracy: 0.9207 - val_loss: 0.5452 - val_accuracy: 0.9118\n",
      "Epoch 151/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5597 - accuracy: 0.9186 - val_loss: 0.5767 - val_accuracy: 0.9031\n",
      "Epoch 152/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.5561 - accuracy: 0.9207 - val_loss: 0.5753 - val_accuracy: 0.9025\n",
      "Epoch 153/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.5491 - accuracy: 0.9221 - val_loss: 0.5426 - val_accuracy: 0.9147\n",
      "Epoch 154/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.5474 - accuracy: 0.9233 - val_loss: 0.5526 - val_accuracy: 0.9151\n",
      "Epoch 155/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5463 - accuracy: 0.9246 - val_loss: 0.5758 - val_accuracy: 0.9048\n",
      "Epoch 156/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5497 - accuracy: 0.9238 - val_loss: 0.5751 - val_accuracy: 0.9000\n",
      "Epoch 157/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5442 - accuracy: 0.9221 - val_loss: 0.5767 - val_accuracy: 0.9037\n",
      "Epoch 158/1000\n",
      "139/139 [==============================] - 14s 96ms/step - loss: 0.5454 - accuracy: 0.9220 - val_loss: 0.5383 - val_accuracy: 0.9166\n",
      "Epoch 159/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5416 - accuracy: 0.9243 - val_loss: 0.5722 - val_accuracy: 0.9046\n",
      "Epoch 160/1000\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 0.5462 - accuracy: 0.9227 - val_loss: 0.5649 - val_accuracy: 0.9100\n",
      "Epoch 161/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.5392 - accuracy: 0.9252 - val_loss: 0.5414 - val_accuracy: 0.9193\n",
      "Epoch 162/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5389 - accuracy: 0.9229 - val_loss: 0.5637 - val_accuracy: 0.9062\n",
      "Epoch 163/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5296 - accuracy: 0.9288 - val_loss: 0.5867 - val_accuracy: 0.9017\n",
      "Epoch 164/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.5355 - accuracy: 0.9267 - val_loss: 0.5628 - val_accuracy: 0.9056\n",
      "Epoch 165/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5377 - accuracy: 0.9227 - val_loss: 0.5718 - val_accuracy: 0.9037\n",
      "Epoch 166/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5349 - accuracy: 0.9246 - val_loss: 0.5454 - val_accuracy: 0.9120\n",
      "Epoch 167/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5326 - accuracy: 0.9245 - val_loss: 0.5637 - val_accuracy: 0.9085\n",
      "Epoch 168/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.5258 - accuracy: 0.9272 - val_loss: 0.5954 - val_accuracy: 0.9029\n",
      "Epoch 169/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.5281 - accuracy: 0.9270 - val_loss: 0.5374 - val_accuracy: 0.9091\n",
      "Epoch 170/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.5280 - accuracy: 0.9279 - val_loss: 0.5649 - val_accuracy: 0.9041\n",
      "Epoch 171/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.5329 - accuracy: 0.9249 - val_loss: 0.5800 - val_accuracy: 0.9079\n",
      "Epoch 172/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.5297 - accuracy: 0.9252 - val_loss: 0.5417 - val_accuracy: 0.9160\n",
      "Epoch 173/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.5301 - accuracy: 0.9255 - val_loss: 0.5377 - val_accuracy: 0.9162\n",
      "Epoch 174/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.5290 - accuracy: 0.9269 - val_loss: 0.5854 - val_accuracy: 0.8965\n",
      "Epoch 175/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5219 - accuracy: 0.9262 - val_loss: 0.5471 - val_accuracy: 0.9108\n",
      "Epoch 176/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5173 - accuracy: 0.9303 - val_loss: 0.5228 - val_accuracy: 0.9228\n",
      "Epoch 177/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5261 - accuracy: 0.9254 - val_loss: 0.5338 - val_accuracy: 0.9147\n",
      "Epoch 178/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5187 - accuracy: 0.9297 - val_loss: 0.5577 - val_accuracy: 0.9052\n",
      "Epoch 179/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5208 - accuracy: 0.9282 - val_loss: 0.5317 - val_accuracy: 0.9210\n",
      "Epoch 180/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5223 - accuracy: 0.9266 - val_loss: 0.5275 - val_accuracy: 0.9162\n",
      "Epoch 181/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.5212 - accuracy: 0.9260 - val_loss: 0.5223 - val_accuracy: 0.9164\n",
      "Epoch 182/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5176 - accuracy: 0.9258 - val_loss: 0.5364 - val_accuracy: 0.9127\n",
      "Epoch 183/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5141 - accuracy: 0.9296 - val_loss: 0.5265 - val_accuracy: 0.9158\n",
      "Epoch 184/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.5175 - accuracy: 0.9249 - val_loss: 0.5307 - val_accuracy: 0.9137\n",
      "Epoch 185/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5151 - accuracy: 0.9301 - val_loss: 0.5617 - val_accuracy: 0.9089\n",
      "Epoch 186/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5129 - accuracy: 0.9287 - val_loss: 0.5168 - val_accuracy: 0.9222\n",
      "Epoch 187/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5111 - accuracy: 0.9296 - val_loss: 0.5494 - val_accuracy: 0.9095\n",
      "Epoch 188/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5051 - accuracy: 0.9314 - val_loss: 0.5348 - val_accuracy: 0.9166\n",
      "Epoch 189/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5117 - accuracy: 0.9293 - val_loss: 0.5296 - val_accuracy: 0.9156\n",
      "Epoch 190/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.5057 - accuracy: 0.9288 - val_loss: 0.5352 - val_accuracy: 0.9154\n",
      "Epoch 191/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.5011 - accuracy: 0.9329 - val_loss: 0.5583 - val_accuracy: 0.9056\n",
      "Epoch 192/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.5063 - accuracy: 0.9312 - val_loss: 0.5859 - val_accuracy: 0.9019\n",
      "Epoch 193/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.5043 - accuracy: 0.9316 - val_loss: 0.5176 - val_accuracy: 0.9207\n",
      "Epoch 194/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.5006 - accuracy: 0.9309 - val_loss: 0.5233 - val_accuracy: 0.9187\n",
      "Epoch 195/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.5073 - accuracy: 0.9279 - val_loss: 0.5323 - val_accuracy: 0.9124\n",
      "Epoch 196/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4995 - accuracy: 0.9339 - val_loss: 0.5290 - val_accuracy: 0.9164\n",
      "Epoch 197/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.5033 - accuracy: 0.9296 - val_loss: 0.5146 - val_accuracy: 0.9214\n",
      "Epoch 198/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.5022 - accuracy: 0.9294 - val_loss: 0.5558 - val_accuracy: 0.9071\n",
      "Epoch 199/1000\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 0.5008 - accuracy: 0.9314 - val_loss: 0.5230 - val_accuracy: 0.9187\n",
      "Epoch 200/1000\n",
      "139/139 [==============================] - 14s 104ms/step - loss: 0.5010 - accuracy: 0.9287 - val_loss: 0.5177 - val_accuracy: 0.9207\n",
      "Epoch 201/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4962 - accuracy: 0.9296 - val_loss: 0.5179 - val_accuracy: 0.9176\n",
      "Epoch 202/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.5004 - accuracy: 0.9312 - val_loss: 0.5203 - val_accuracy: 0.9178\n",
      "Epoch 203/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.5021 - accuracy: 0.9308 - val_loss: 0.5101 - val_accuracy: 0.9180\n",
      "Epoch 204/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4951 - accuracy: 0.9312 - val_loss: 0.5410 - val_accuracy: 0.9131\n",
      "Epoch 205/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4988 - accuracy: 0.9327 - val_loss: 0.5205 - val_accuracy: 0.9149\n",
      "Epoch 206/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4945 - accuracy: 0.9318 - val_loss: 0.5033 - val_accuracy: 0.9212\n",
      "Epoch 207/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4924 - accuracy: 0.9301 - val_loss: 0.5429 - val_accuracy: 0.9046\n",
      "Epoch 208/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4953 - accuracy: 0.9309 - val_loss: 0.5259 - val_accuracy: 0.9176\n",
      "Epoch 209/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4935 - accuracy: 0.9330 - val_loss: 0.5172 - val_accuracy: 0.9160\n",
      "Epoch 210/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4865 - accuracy: 0.9345 - val_loss: 0.5844 - val_accuracy: 0.8967\n",
      "Epoch 211/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4958 - accuracy: 0.9332 - val_loss: 0.5528 - val_accuracy: 0.9066\n",
      "Epoch 212/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4905 - accuracy: 0.9347 - val_loss: 0.5233 - val_accuracy: 0.9154\n",
      "Epoch 213/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4888 - accuracy: 0.9336 - val_loss: 0.5172 - val_accuracy: 0.9149\n",
      "Epoch 214/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4885 - accuracy: 0.9330 - val_loss: 0.5414 - val_accuracy: 0.9118\n",
      "Epoch 215/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4850 - accuracy: 0.9335 - val_loss: 0.5103 - val_accuracy: 0.9216\n",
      "Epoch 216/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4868 - accuracy: 0.9341 - val_loss: 0.5020 - val_accuracy: 0.9222\n",
      "Epoch 217/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4858 - accuracy: 0.9326 - val_loss: 0.5109 - val_accuracy: 0.9220\n",
      "Epoch 218/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4841 - accuracy: 0.9337 - val_loss: 0.5235 - val_accuracy: 0.9120\n",
      "Epoch 219/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4867 - accuracy: 0.9332 - val_loss: 0.5092 - val_accuracy: 0.9228\n",
      "Epoch 220/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4829 - accuracy: 0.9327 - val_loss: 0.5056 - val_accuracy: 0.9203\n",
      "Epoch 221/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4821 - accuracy: 0.9332 - val_loss: 0.5184 - val_accuracy: 0.9127\n",
      "Epoch 222/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4866 - accuracy: 0.9314 - val_loss: 0.4973 - val_accuracy: 0.9241\n",
      "Epoch 223/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4835 - accuracy: 0.9332 - val_loss: 0.5172 - val_accuracy: 0.9170\n",
      "Epoch 224/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4792 - accuracy: 0.9326 - val_loss: 0.5095 - val_accuracy: 0.9203\n",
      "Epoch 225/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4826 - accuracy: 0.9335 - val_loss: 0.5034 - val_accuracy: 0.9224\n",
      "Epoch 226/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4807 - accuracy: 0.9332 - val_loss: 0.5078 - val_accuracy: 0.9166\n",
      "Epoch 227/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4760 - accuracy: 0.9355 - val_loss: 0.5156 - val_accuracy: 0.9212\n",
      "Epoch 228/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4742 - accuracy: 0.9357 - val_loss: 0.4987 - val_accuracy: 0.9210\n",
      "Epoch 229/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4786 - accuracy: 0.9357 - val_loss: 0.4933 - val_accuracy: 0.9263\n",
      "Epoch 230/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4776 - accuracy: 0.9347 - val_loss: 0.5293 - val_accuracy: 0.9191\n",
      "Epoch 231/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4796 - accuracy: 0.9351 - val_loss: 0.5071 - val_accuracy: 0.9174\n",
      "Epoch 232/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4734 - accuracy: 0.9388 - val_loss: 0.4995 - val_accuracy: 0.9185\n",
      "Epoch 233/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4700 - accuracy: 0.9379 - val_loss: 0.5250 - val_accuracy: 0.9145\n",
      "Epoch 234/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4710 - accuracy: 0.9387 - val_loss: 0.5042 - val_accuracy: 0.9222\n",
      "Epoch 235/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4729 - accuracy: 0.9365 - val_loss: 0.4977 - val_accuracy: 0.9189\n",
      "Epoch 236/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4672 - accuracy: 0.9401 - val_loss: 0.4911 - val_accuracy: 0.9261\n",
      "Epoch 237/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4682 - accuracy: 0.9370 - val_loss: 0.5509 - val_accuracy: 0.9023\n",
      "Epoch 238/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4711 - accuracy: 0.9370 - val_loss: 0.4997 - val_accuracy: 0.9170\n",
      "Epoch 239/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4826 - accuracy: 0.9312 - val_loss: 0.5005 - val_accuracy: 0.9166\n",
      "Epoch 240/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4691 - accuracy: 0.9396 - val_loss: 0.4963 - val_accuracy: 0.9172\n",
      "Epoch 241/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4702 - accuracy: 0.9365 - val_loss: 0.4994 - val_accuracy: 0.9191\n",
      "Epoch 242/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4712 - accuracy: 0.9374 - val_loss: 0.5085 - val_accuracy: 0.9191\n",
      "Epoch 243/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4630 - accuracy: 0.9399 - val_loss: 0.5082 - val_accuracy: 0.9164\n",
      "Epoch 244/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4688 - accuracy: 0.9361 - val_loss: 0.5110 - val_accuracy: 0.9158\n",
      "Epoch 245/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4705 - accuracy: 0.9363 - val_loss: 0.4946 - val_accuracy: 0.9189\n",
      "Epoch 246/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4675 - accuracy: 0.9361 - val_loss: 0.4861 - val_accuracy: 0.9239\n",
      "Epoch 247/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4657 - accuracy: 0.9380 - val_loss: 0.5260 - val_accuracy: 0.9139\n",
      "Epoch 248/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4609 - accuracy: 0.9378 - val_loss: 0.4936 - val_accuracy: 0.9170\n",
      "Epoch 249/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4620 - accuracy: 0.9388 - val_loss: 0.4932 - val_accuracy: 0.9205\n",
      "Epoch 250/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4565 - accuracy: 0.9386 - val_loss: 0.4978 - val_accuracy: 0.9195\n",
      "Epoch 251/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4638 - accuracy: 0.9354 - val_loss: 0.4930 - val_accuracy: 0.9261\n",
      "Epoch 252/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4622 - accuracy: 0.9373 - val_loss: 0.5274 - val_accuracy: 0.9122\n",
      "Epoch 253/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4668 - accuracy: 0.9357 - val_loss: 0.4994 - val_accuracy: 0.9183\n",
      "Epoch 254/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4607 - accuracy: 0.9383 - val_loss: 0.5130 - val_accuracy: 0.9127\n",
      "Epoch 255/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4564 - accuracy: 0.9378 - val_loss: 0.5211 - val_accuracy: 0.9162\n",
      "Epoch 256/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.4586 - accuracy: 0.9393 - val_loss: 0.4876 - val_accuracy: 0.9245\n",
      "Epoch 257/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4579 - accuracy: 0.9395 - val_loss: 0.5239 - val_accuracy: 0.9129\n",
      "Epoch 258/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4599 - accuracy: 0.9395 - val_loss: 0.4940 - val_accuracy: 0.9272\n",
      "Epoch 259/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4554 - accuracy: 0.9361 - val_loss: 0.4943 - val_accuracy: 0.9216\n",
      "Epoch 260/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.4585 - accuracy: 0.9375 - val_loss: 0.4967 - val_accuracy: 0.9176\n",
      "Epoch 261/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4571 - accuracy: 0.9372 - val_loss: 0.4860 - val_accuracy: 0.9249\n",
      "Epoch 262/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4565 - accuracy: 0.9397 - val_loss: 0.4874 - val_accuracy: 0.9234\n",
      "Epoch 263/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4559 - accuracy: 0.9398 - val_loss: 0.4994 - val_accuracy: 0.9164\n",
      "Epoch 264/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4528 - accuracy: 0.9400 - val_loss: 0.4908 - val_accuracy: 0.9245\n",
      "Epoch 265/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4540 - accuracy: 0.9401 - val_loss: 0.5073 - val_accuracy: 0.9172\n",
      "Epoch 266/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4567 - accuracy: 0.9384 - val_loss: 0.4883 - val_accuracy: 0.9237\n",
      "Epoch 267/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4499 - accuracy: 0.9414 - val_loss: 0.4857 - val_accuracy: 0.9203\n",
      "Epoch 268/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4506 - accuracy: 0.9392 - val_loss: 0.4972 - val_accuracy: 0.9174\n",
      "Epoch 269/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4525 - accuracy: 0.9387 - val_loss: 0.5054 - val_accuracy: 0.9158\n",
      "Epoch 270/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4499 - accuracy: 0.9397 - val_loss: 0.4921 - val_accuracy: 0.9210\n",
      "Epoch 271/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4504 - accuracy: 0.9384 - val_loss: 0.4880 - val_accuracy: 0.9203\n",
      "Epoch 272/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4543 - accuracy: 0.9405 - val_loss: 0.4920 - val_accuracy: 0.9224\n",
      "Epoch 273/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4472 - accuracy: 0.9418 - val_loss: 0.5003 - val_accuracy: 0.9154\n",
      "Epoch 274/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4470 - accuracy: 0.9419 - val_loss: 0.4811 - val_accuracy: 0.9237\n",
      "Epoch 275/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4459 - accuracy: 0.9452 - val_loss: 0.5008 - val_accuracy: 0.9195\n",
      "Epoch 276/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4515 - accuracy: 0.9401 - val_loss: 0.4909 - val_accuracy: 0.9220\n",
      "Epoch 277/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4465 - accuracy: 0.9400 - val_loss: 0.4820 - val_accuracy: 0.9241\n",
      "Epoch 278/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4476 - accuracy: 0.9402 - val_loss: 0.4918 - val_accuracy: 0.9216\n",
      "Epoch 279/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4464 - accuracy: 0.9409 - val_loss: 0.4956 - val_accuracy: 0.9176\n",
      "Epoch 280/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4454 - accuracy: 0.9397 - val_loss: 0.5039 - val_accuracy: 0.9187\n",
      "Epoch 281/1000\n",
      "139/139 [==============================] - 13s 93ms/step - loss: 0.4407 - accuracy: 0.9464 - val_loss: 0.5175 - val_accuracy: 0.9189\n",
      "Epoch 282/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4448 - accuracy: 0.9400 - val_loss: 0.4912 - val_accuracy: 0.9230\n",
      "Epoch 283/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4450 - accuracy: 0.9423 - val_loss: 0.5167 - val_accuracy: 0.9127\n",
      "Epoch 284/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4447 - accuracy: 0.9387 - val_loss: 0.5220 - val_accuracy: 0.9114\n",
      "Epoch 285/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4537 - accuracy: 0.9366 - val_loss: 0.4786 - val_accuracy: 0.9228\n",
      "Epoch 286/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4482 - accuracy: 0.9380 - val_loss: 0.5015 - val_accuracy: 0.9160\n",
      "Epoch 287/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4422 - accuracy: 0.9415 - val_loss: 0.4739 - val_accuracy: 0.9268\n",
      "Epoch 288/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4389 - accuracy: 0.9454 - val_loss: 0.4934 - val_accuracy: 0.9207\n",
      "Epoch 289/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4434 - accuracy: 0.9417 - val_loss: 0.4859 - val_accuracy: 0.9295\n",
      "Epoch 290/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4394 - accuracy: 0.9441 - val_loss: 0.4803 - val_accuracy: 0.9245\n",
      "Epoch 291/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4423 - accuracy: 0.9414 - val_loss: 0.4792 - val_accuracy: 0.9247\n",
      "Epoch 292/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4445 - accuracy: 0.9419 - val_loss: 0.4774 - val_accuracy: 0.9255\n",
      "Epoch 293/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4421 - accuracy: 0.9417 - val_loss: 0.5069 - val_accuracy: 0.9160\n",
      "Epoch 294/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4416 - accuracy: 0.9402 - val_loss: 0.4995 - val_accuracy: 0.9197\n",
      "Epoch 295/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4423 - accuracy: 0.9416 - val_loss: 0.4797 - val_accuracy: 0.9237\n",
      "Epoch 296/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4342 - accuracy: 0.9416 - val_loss: 0.5020 - val_accuracy: 0.9139\n",
      "Epoch 297/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4365 - accuracy: 0.9426 - val_loss: 0.4865 - val_accuracy: 0.9189\n",
      "Epoch 298/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4377 - accuracy: 0.9446 - val_loss: 0.4721 - val_accuracy: 0.9201\n",
      "Epoch 299/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4357 - accuracy: 0.9453 - val_loss: 0.4897 - val_accuracy: 0.9224\n",
      "Epoch 300/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4357 - accuracy: 0.9443 - val_loss: 0.5154 - val_accuracy: 0.9120\n",
      "Epoch 301/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4390 - accuracy: 0.9422 - val_loss: 0.4846 - val_accuracy: 0.9253\n",
      "Epoch 302/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4298 - accuracy: 0.9469 - val_loss: 0.4899 - val_accuracy: 0.9180\n",
      "Epoch 303/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4335 - accuracy: 0.9443 - val_loss: 0.4846 - val_accuracy: 0.9203\n",
      "Epoch 304/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4351 - accuracy: 0.9405 - val_loss: 0.4727 - val_accuracy: 0.9239\n",
      "Epoch 305/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4348 - accuracy: 0.9442 - val_loss: 0.4895 - val_accuracy: 0.9189\n",
      "Epoch 306/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4324 - accuracy: 0.9438 - val_loss: 0.4721 - val_accuracy: 0.9261\n",
      "Epoch 307/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4301 - accuracy: 0.9461 - val_loss: 0.4754 - val_accuracy: 0.9247\n",
      "Epoch 308/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4344 - accuracy: 0.9415 - val_loss: 0.4733 - val_accuracy: 0.9214\n",
      "Epoch 309/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4344 - accuracy: 0.9428 - val_loss: 0.4730 - val_accuracy: 0.9257\n",
      "Epoch 310/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4363 - accuracy: 0.9431 - val_loss: 0.4798 - val_accuracy: 0.9239\n",
      "Epoch 311/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4323 - accuracy: 0.9450 - val_loss: 0.4828 - val_accuracy: 0.9207\n",
      "Epoch 312/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4323 - accuracy: 0.9442 - val_loss: 0.4718 - val_accuracy: 0.9241\n",
      "Epoch 313/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4329 - accuracy: 0.9420 - val_loss: 0.4790 - val_accuracy: 0.9218\n",
      "Epoch 314/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4315 - accuracy: 0.9424 - val_loss: 0.5124 - val_accuracy: 0.9116\n",
      "Epoch 315/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4299 - accuracy: 0.9451 - val_loss: 0.4865 - val_accuracy: 0.9187\n",
      "Epoch 316/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4303 - accuracy: 0.9436 - val_loss: 0.4811 - val_accuracy: 0.9228\n",
      "Epoch 317/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4333 - accuracy: 0.9411 - val_loss: 0.4773 - val_accuracy: 0.9234\n",
      "Epoch 318/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4293 - accuracy: 0.9433 - val_loss: 0.4769 - val_accuracy: 0.9239\n",
      "Epoch 319/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4256 - accuracy: 0.9460 - val_loss: 0.4813 - val_accuracy: 0.9214\n",
      "Epoch 320/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4244 - accuracy: 0.9462 - val_loss: 0.4854 - val_accuracy: 0.9222\n",
      "Epoch 321/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4290 - accuracy: 0.9428 - val_loss: 0.4775 - val_accuracy: 0.9226\n",
      "Epoch 322/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4288 - accuracy: 0.9417 - val_loss: 0.4759 - val_accuracy: 0.9245\n",
      "Epoch 323/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4258 - accuracy: 0.9451 - val_loss: 0.4835 - val_accuracy: 0.9282\n",
      "Epoch 324/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4261 - accuracy: 0.9432 - val_loss: 0.4870 - val_accuracy: 0.9178\n",
      "Epoch 325/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4289 - accuracy: 0.9426 - val_loss: 0.4915 - val_accuracy: 0.9154\n",
      "Epoch 326/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4245 - accuracy: 0.9469 - val_loss: 0.4790 - val_accuracy: 0.9195\n",
      "Epoch 327/1000\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 0.4276 - accuracy: 0.9423 - val_loss: 0.4722 - val_accuracy: 0.9230\n",
      "Epoch 328/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4187 - accuracy: 0.9455 - val_loss: 0.5050 - val_accuracy: 0.9160\n",
      "Epoch 329/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4244 - accuracy: 0.9474 - val_loss: 0.4674 - val_accuracy: 0.9272\n",
      "Epoch 330/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4241 - accuracy: 0.9443 - val_loss: 0.4945 - val_accuracy: 0.9164\n",
      "Epoch 331/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4243 - accuracy: 0.9445 - val_loss: 0.4831 - val_accuracy: 0.9239\n",
      "Epoch 332/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4273 - accuracy: 0.9440 - val_loss: 0.4629 - val_accuracy: 0.9257\n",
      "Epoch 333/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4261 - accuracy: 0.9436 - val_loss: 0.5192 - val_accuracy: 0.9056\n",
      "Epoch 334/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4236 - accuracy: 0.9431 - val_loss: 0.4649 - val_accuracy: 0.9270\n",
      "Epoch 335/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.4215 - accuracy: 0.9460 - val_loss: 0.4824 - val_accuracy: 0.9212\n",
      "Epoch 336/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4214 - accuracy: 0.9453 - val_loss: 0.4976 - val_accuracy: 0.9149\n",
      "Epoch 337/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4254 - accuracy: 0.9433 - val_loss: 0.4767 - val_accuracy: 0.9214\n",
      "Epoch 338/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4245 - accuracy: 0.9427 - val_loss: 0.4786 - val_accuracy: 0.9224\n",
      "Epoch 339/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4214 - accuracy: 0.9461 - val_loss: 0.4802 - val_accuracy: 0.9212\n",
      "Epoch 340/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4176 - accuracy: 0.9479 - val_loss: 0.4919 - val_accuracy: 0.9174\n",
      "Epoch 341/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4239 - accuracy: 0.9415 - val_loss: 0.5003 - val_accuracy: 0.9139\n",
      "Epoch 342/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4231 - accuracy: 0.9446 - val_loss: 0.4770 - val_accuracy: 0.9205\n",
      "Epoch 343/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4227 - accuracy: 0.9416 - val_loss: 0.4846 - val_accuracy: 0.9178\n",
      "Epoch 344/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4239 - accuracy: 0.9435 - val_loss: 0.4665 - val_accuracy: 0.9268\n",
      "Epoch 345/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4145 - accuracy: 0.9499 - val_loss: 0.4754 - val_accuracy: 0.9201\n",
      "Epoch 346/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4185 - accuracy: 0.9441 - val_loss: 0.4848 - val_accuracy: 0.9170\n",
      "Epoch 347/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4115 - accuracy: 0.9526 - val_loss: 0.4729 - val_accuracy: 0.9191\n",
      "Epoch 348/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4204 - accuracy: 0.9462 - val_loss: 0.4778 - val_accuracy: 0.9214\n",
      "Epoch 349/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4131 - accuracy: 0.9492 - val_loss: 0.4770 - val_accuracy: 0.9195\n",
      "Epoch 350/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4198 - accuracy: 0.9441 - val_loss: 0.4863 - val_accuracy: 0.9185\n",
      "Epoch 351/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4148 - accuracy: 0.9477 - val_loss: 0.4665 - val_accuracy: 0.9195\n",
      "Epoch 352/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.4137 - accuracy: 0.9485 - val_loss: 0.4783 - val_accuracy: 0.9220\n",
      "Epoch 353/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4197 - accuracy: 0.9440 - val_loss: 0.4897 - val_accuracy: 0.9207\n",
      "Epoch 354/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4134 - accuracy: 0.9458 - val_loss: 0.4960 - val_accuracy: 0.9183\n",
      "Epoch 355/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4100 - accuracy: 0.9478 - val_loss: 0.4719 - val_accuracy: 0.9249\n",
      "Epoch 356/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4116 - accuracy: 0.9486 - val_loss: 0.4716 - val_accuracy: 0.9220\n",
      "Epoch 357/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4135 - accuracy: 0.9459 - val_loss: 0.4809 - val_accuracy: 0.9218\n",
      "Epoch 358/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.4197 - accuracy: 0.9452 - val_loss: 0.4784 - val_accuracy: 0.9166\n",
      "Epoch 359/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4134 - accuracy: 0.9451 - val_loss: 0.4813 - val_accuracy: 0.9245\n",
      "Epoch 360/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4151 - accuracy: 0.9459 - val_loss: 0.4764 - val_accuracy: 0.9216\n",
      "Epoch 361/1000\n",
      "139/139 [==============================] - 14s 104ms/step - loss: 0.4123 - accuracy: 0.9467 - val_loss: 0.4614 - val_accuracy: 0.9249\n",
      "Epoch 362/1000\n",
      "139/139 [==============================] - 15s 105ms/step - loss: 0.4086 - accuracy: 0.9492 - val_loss: 0.5032 - val_accuracy: 0.9114\n",
      "Epoch 363/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.4129 - accuracy: 0.9460 - val_loss: 0.4675 - val_accuracy: 0.9268\n",
      "Epoch 364/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4089 - accuracy: 0.9504 - val_loss: 0.4862 - val_accuracy: 0.9168\n",
      "Epoch 365/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.4118 - accuracy: 0.9477 - val_loss: 0.4810 - val_accuracy: 0.9201\n",
      "Epoch 366/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4089 - accuracy: 0.9470 - val_loss: 0.4656 - val_accuracy: 0.9228\n",
      "Epoch 367/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4119 - accuracy: 0.9447 - val_loss: 0.5039 - val_accuracy: 0.9079\n",
      "Epoch 368/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4107 - accuracy: 0.9456 - val_loss: 0.4782 - val_accuracy: 0.9201\n",
      "Epoch 369/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4066 - accuracy: 0.9490 - val_loss: 0.4748 - val_accuracy: 0.9203\n",
      "Epoch 370/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4100 - accuracy: 0.9471 - val_loss: 0.4745 - val_accuracy: 0.9234\n",
      "Epoch 371/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4042 - accuracy: 0.9503 - val_loss: 0.4646 - val_accuracy: 0.9274\n",
      "Epoch 372/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4136 - accuracy: 0.9482 - val_loss: 0.4876 - val_accuracy: 0.9178\n",
      "Epoch 373/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4103 - accuracy: 0.9460 - val_loss: 0.4559 - val_accuracy: 0.9230\n",
      "Epoch 374/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4073 - accuracy: 0.9477 - val_loss: 0.5009 - val_accuracy: 0.9106\n",
      "Epoch 375/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4119 - accuracy: 0.9444 - val_loss: 0.4955 - val_accuracy: 0.9195\n",
      "Epoch 376/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4058 - accuracy: 0.9497 - val_loss: 0.4742 - val_accuracy: 0.9205\n",
      "Epoch 377/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4096 - accuracy: 0.9485 - val_loss: 0.4578 - val_accuracy: 0.9261\n",
      "Epoch 378/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.4043 - accuracy: 0.9492 - val_loss: 0.4807 - val_accuracy: 0.9176\n",
      "Epoch 379/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4057 - accuracy: 0.9482 - val_loss: 0.4625 - val_accuracy: 0.9230\n",
      "Epoch 380/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4068 - accuracy: 0.9481 - val_loss: 0.4833 - val_accuracy: 0.9195\n",
      "Epoch 381/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4090 - accuracy: 0.9481 - val_loss: 0.4873 - val_accuracy: 0.9172\n",
      "Epoch 382/1000\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 0.4036 - accuracy: 0.9480 - val_loss: 0.4644 - val_accuracy: 0.9234\n",
      "Epoch 383/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4146 - accuracy: 0.9449 - val_loss: 0.4685 - val_accuracy: 0.9174\n",
      "Epoch 384/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4035 - accuracy: 0.9495 - val_loss: 0.4710 - val_accuracy: 0.9210\n",
      "Epoch 385/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4067 - accuracy: 0.9479 - val_loss: 0.4701 - val_accuracy: 0.9230\n",
      "Epoch 386/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.4072 - accuracy: 0.9478 - val_loss: 0.4784 - val_accuracy: 0.9164\n",
      "Epoch 387/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4028 - accuracy: 0.9503 - val_loss: 0.4828 - val_accuracy: 0.9178\n",
      "Epoch 388/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4020 - accuracy: 0.9491 - val_loss: 0.4628 - val_accuracy: 0.9247\n",
      "Epoch 389/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.4041 - accuracy: 0.9478 - val_loss: 0.4724 - val_accuracy: 0.9199\n",
      "Epoch 390/1000\n",
      "139/139 [==============================] - 14s 104ms/step - loss: 0.3990 - accuracy: 0.9487 - val_loss: 0.4835 - val_accuracy: 0.9228\n",
      "Epoch 391/1000\n",
      "139/139 [==============================] - 17s 119ms/step - loss: 0.3997 - accuracy: 0.9480 - val_loss: 0.4982 - val_accuracy: 0.9203\n",
      "Epoch 392/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.4052 - accuracy: 0.9449 - val_loss: 0.4496 - val_accuracy: 0.9243\n",
      "Epoch 393/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4015 - accuracy: 0.9478 - val_loss: 0.4795 - val_accuracy: 0.9205\n",
      "Epoch 394/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3996 - accuracy: 0.9491 - val_loss: 0.4761 - val_accuracy: 0.9197\n",
      "Epoch 395/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4064 - accuracy: 0.9476 - val_loss: 0.4627 - val_accuracy: 0.9247\n",
      "Epoch 396/1000\n",
      "139/139 [==============================] - 16s 116ms/step - loss: 0.4062 - accuracy: 0.9482 - val_loss: 0.4807 - val_accuracy: 0.9234\n",
      "Epoch 397/1000\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 0.4010 - accuracy: 0.9513 - val_loss: 0.4569 - val_accuracy: 0.9237\n",
      "Epoch 398/1000\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 0.3990 - accuracy: 0.9516 - val_loss: 0.4660 - val_accuracy: 0.9249\n",
      "Epoch 399/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.4008 - accuracy: 0.9505 - val_loss: 0.4638 - val_accuracy: 0.9220\n",
      "Epoch 400/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4002 - accuracy: 0.9485 - val_loss: 0.4692 - val_accuracy: 0.9185\n",
      "Epoch 401/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.4024 - accuracy: 0.9469 - val_loss: 0.4677 - val_accuracy: 0.9195\n",
      "Epoch 402/1000\n",
      "139/139 [==============================] - 15s 105ms/step - loss: 0.4030 - accuracy: 0.9476 - val_loss: 0.4703 - val_accuracy: 0.9201\n",
      "Epoch 403/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3983 - accuracy: 0.9492 - val_loss: 0.4745 - val_accuracy: 0.9191\n",
      "Epoch 404/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.4001 - accuracy: 0.9477 - val_loss: 0.4735 - val_accuracy: 0.9174\n",
      "Epoch 405/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3993 - accuracy: 0.9498 - val_loss: 0.4694 - val_accuracy: 0.9259\n",
      "Epoch 406/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.4018 - accuracy: 0.9461 - val_loss: 0.4568 - val_accuracy: 0.9249\n",
      "Epoch 407/1000\n",
      "139/139 [==============================] - 16s 113ms/step - loss: 0.3944 - accuracy: 0.9513 - val_loss: 0.4640 - val_accuracy: 0.9203\n",
      "Epoch 408/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.4022 - accuracy: 0.9488 - val_loss: 0.4590 - val_accuracy: 0.9216\n",
      "Epoch 409/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.3965 - accuracy: 0.9499 - val_loss: 0.4623 - val_accuracy: 0.9247\n",
      "Epoch 410/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3928 - accuracy: 0.9525 - val_loss: 0.4522 - val_accuracy: 0.9272\n",
      "Epoch 411/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3938 - accuracy: 0.9496 - val_loss: 0.4806 - val_accuracy: 0.9216\n",
      "Epoch 412/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3917 - accuracy: 0.9510 - val_loss: 0.4646 - val_accuracy: 0.9234\n",
      "Epoch 413/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3966 - accuracy: 0.9485 - val_loss: 0.4789 - val_accuracy: 0.9172\n",
      "Epoch 414/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.3947 - accuracy: 0.9480 - val_loss: 0.4754 - val_accuracy: 0.9191\n",
      "Epoch 415/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3989 - accuracy: 0.9474 - val_loss: 0.4585 - val_accuracy: 0.9207\n",
      "Epoch 416/1000\n",
      "139/139 [==============================] - 14s 102ms/step - loss: 0.3959 - accuracy: 0.9487 - val_loss: 0.4534 - val_accuracy: 0.9270\n",
      "Epoch 417/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3906 - accuracy: 0.9510 - val_loss: 0.4625 - val_accuracy: 0.9259\n",
      "Epoch 418/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3978 - accuracy: 0.9469 - val_loss: 0.4678 - val_accuracy: 0.9210\n",
      "Epoch 419/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.3955 - accuracy: 0.9492 - val_loss: 0.4588 - val_accuracy: 0.9253\n",
      "Epoch 420/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.3958 - accuracy: 0.9491 - val_loss: 0.4602 - val_accuracy: 0.9241\n",
      "Epoch 421/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3966 - accuracy: 0.9497 - val_loss: 0.4596 - val_accuracy: 0.9214\n",
      "Epoch 422/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3969 - accuracy: 0.9499 - val_loss: 0.4740 - val_accuracy: 0.9162\n",
      "Epoch 423/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.4004 - accuracy: 0.9449 - val_loss: 0.4905 - val_accuracy: 0.9160\n",
      "Epoch 424/1000\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 0.3917 - accuracy: 0.9504 - val_loss: 0.4686 - val_accuracy: 0.9199\n",
      "Epoch 425/1000\n",
      "139/139 [==============================] - 15s 110ms/step - loss: 0.3957 - accuracy: 0.9479 - val_loss: 0.4550 - val_accuracy: 0.9255\n",
      "Epoch 426/1000\n",
      "139/139 [==============================] - 14s 104ms/step - loss: 0.3925 - accuracy: 0.9522 - val_loss: 0.4917 - val_accuracy: 0.9131\n",
      "Epoch 427/1000\n",
      "139/139 [==============================] - 14s 103ms/step - loss: 0.3917 - accuracy: 0.9518 - val_loss: 0.4622 - val_accuracy: 0.9193\n",
      "Epoch 428/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3948 - accuracy: 0.9519 - val_loss: 0.4618 - val_accuracy: 0.9226\n",
      "Epoch 429/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3932 - accuracy: 0.9494 - val_loss: 0.4566 - val_accuracy: 0.9226\n",
      "Epoch 430/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.3928 - accuracy: 0.9501 - val_loss: 0.4755 - val_accuracy: 0.9176\n",
      "Epoch 431/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.3884 - accuracy: 0.9523 - val_loss: 0.4690 - val_accuracy: 0.9189\n",
      "Epoch 432/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3893 - accuracy: 0.9501 - val_loss: 0.4713 - val_accuracy: 0.9185\n",
      "Epoch 433/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3874 - accuracy: 0.9540 - val_loss: 0.4510 - val_accuracy: 0.9241\n",
      "Epoch 434/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3917 - accuracy: 0.9498 - val_loss: 0.4691 - val_accuracy: 0.9212\n",
      "Epoch 435/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3935 - accuracy: 0.9492 - val_loss: 0.4507 - val_accuracy: 0.9259\n",
      "Epoch 436/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.3921 - accuracy: 0.9495 - val_loss: 0.4562 - val_accuracy: 0.9253\n",
      "Epoch 437/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3868 - accuracy: 0.9487 - val_loss: 0.4564 - val_accuracy: 0.9232\n",
      "Epoch 438/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.3859 - accuracy: 0.9536 - val_loss: 0.4638 - val_accuracy: 0.9205\n",
      "Epoch 439/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3886 - accuracy: 0.9545 - val_loss: 0.4565 - val_accuracy: 0.9228\n",
      "Epoch 440/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3832 - accuracy: 0.9522 - val_loss: 0.4664 - val_accuracy: 0.9224\n",
      "Epoch 441/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3928 - accuracy: 0.9491 - val_loss: 0.4763 - val_accuracy: 0.9168\n",
      "Epoch 442/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3874 - accuracy: 0.9512 - val_loss: 0.4539 - val_accuracy: 0.9224\n",
      "Epoch 443/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3903 - accuracy: 0.9504 - val_loss: 0.4675 - val_accuracy: 0.9193\n",
      "Epoch 444/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3879 - accuracy: 0.9506 - val_loss: 0.4677 - val_accuracy: 0.9239\n",
      "Epoch 445/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3895 - accuracy: 0.9495 - val_loss: 0.4676 - val_accuracy: 0.9176\n",
      "Epoch 446/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3808 - accuracy: 0.9551 - val_loss: 0.4505 - val_accuracy: 0.9259\n",
      "Epoch 447/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3849 - accuracy: 0.9534 - val_loss: 0.4645 - val_accuracy: 0.9174\n",
      "Epoch 448/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3954 - accuracy: 0.9480 - val_loss: 0.4691 - val_accuracy: 0.9207\n",
      "Epoch 449/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.3855 - accuracy: 0.9521 - val_loss: 0.4669 - val_accuracy: 0.9205\n",
      "Epoch 450/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3874 - accuracy: 0.9491 - val_loss: 0.4605 - val_accuracy: 0.9210\n",
      "Epoch 451/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3849 - accuracy: 0.9510 - val_loss: 0.4568 - val_accuracy: 0.9261\n",
      "Epoch 452/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3801 - accuracy: 0.9537 - val_loss: 0.4591 - val_accuracy: 0.9207\n",
      "Epoch 453/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3887 - accuracy: 0.9495 - val_loss: 0.4559 - val_accuracy: 0.9199\n",
      "Epoch 454/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.3847 - accuracy: 0.9512 - val_loss: 0.4684 - val_accuracy: 0.9183\n",
      "Epoch 455/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.3860 - accuracy: 0.9510 - val_loss: 0.4693 - val_accuracy: 0.9164\n",
      "Epoch 456/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3826 - accuracy: 0.9507 - val_loss: 0.4536 - val_accuracy: 0.9247\n",
      "Epoch 457/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3826 - accuracy: 0.9513 - val_loss: 0.4570 - val_accuracy: 0.9199\n",
      "Epoch 458/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3796 - accuracy: 0.9531 - val_loss: 0.4520 - val_accuracy: 0.9212\n",
      "Epoch 459/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3829 - accuracy: 0.9516 - val_loss: 0.4529 - val_accuracy: 0.9241\n",
      "Epoch 460/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3838 - accuracy: 0.9527 - val_loss: 0.4734 - val_accuracy: 0.9160\n",
      "Epoch 461/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3860 - accuracy: 0.9494 - val_loss: 0.4664 - val_accuracy: 0.9241\n",
      "Epoch 462/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3894 - accuracy: 0.9491 - val_loss: 0.4608 - val_accuracy: 0.9183\n",
      "Epoch 463/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3801 - accuracy: 0.9521 - val_loss: 0.4507 - val_accuracy: 0.9241\n",
      "Epoch 464/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3838 - accuracy: 0.9505 - val_loss: 0.4586 - val_accuracy: 0.9180\n",
      "Epoch 465/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3819 - accuracy: 0.9505 - val_loss: 0.4652 - val_accuracy: 0.9216\n",
      "Epoch 466/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3858 - accuracy: 0.9506 - val_loss: 0.4580 - val_accuracy: 0.9234\n",
      "Epoch 467/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3798 - accuracy: 0.9533 - val_loss: 0.4534 - val_accuracy: 0.9239\n",
      "Epoch 468/1000\n",
      "139/139 [==============================] - 14s 100ms/step - loss: 0.3829 - accuracy: 0.9509 - val_loss: 0.4628 - val_accuracy: 0.9243\n",
      "Epoch 469/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.3819 - accuracy: 0.9526 - val_loss: 0.4510 - val_accuracy: 0.9228\n",
      "Epoch 470/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3813 - accuracy: 0.9516 - val_loss: 0.4483 - val_accuracy: 0.9266\n",
      "Epoch 471/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3816 - accuracy: 0.9510 - val_loss: 0.4506 - val_accuracy: 0.9228\n",
      "Epoch 472/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3812 - accuracy: 0.9517 - val_loss: 0.4483 - val_accuracy: 0.9268\n",
      "Epoch 473/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3808 - accuracy: 0.9513 - val_loss: 0.4561 - val_accuracy: 0.9218\n",
      "Epoch 474/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3830 - accuracy: 0.9524 - val_loss: 0.4518 - val_accuracy: 0.9232\n",
      "Epoch 475/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3802 - accuracy: 0.9512 - val_loss: 0.4499 - val_accuracy: 0.9249\n",
      "Epoch 476/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3813 - accuracy: 0.9517 - val_loss: 0.4583 - val_accuracy: 0.9216\n",
      "Epoch 477/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3773 - accuracy: 0.9532 - val_loss: 0.4465 - val_accuracy: 0.9293\n",
      "Epoch 478/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3797 - accuracy: 0.9530 - val_loss: 0.4568 - val_accuracy: 0.9234\n",
      "Epoch 479/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3788 - accuracy: 0.9528 - val_loss: 0.4641 - val_accuracy: 0.9237\n",
      "Epoch 480/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3804 - accuracy: 0.9510 - val_loss: 0.4436 - val_accuracy: 0.9247\n",
      "Epoch 481/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3798 - accuracy: 0.9527 - val_loss: 0.4478 - val_accuracy: 0.9239\n",
      "Epoch 482/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3817 - accuracy: 0.9480 - val_loss: 0.4535 - val_accuracy: 0.9276\n",
      "Epoch 483/1000\n",
      "139/139 [==============================] - 14s 97ms/step - loss: 0.3786 - accuracy: 0.9518 - val_loss: 0.4591 - val_accuracy: 0.9228\n",
      "Epoch 484/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3816 - accuracy: 0.9490 - val_loss: 0.4486 - val_accuracy: 0.9239\n",
      "Epoch 485/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3744 - accuracy: 0.9557 - val_loss: 0.4661 - val_accuracy: 0.9203\n",
      "Epoch 486/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.3755 - accuracy: 0.9541 - val_loss: 0.4500 - val_accuracy: 0.9224\n",
      "Epoch 487/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3740 - accuracy: 0.9530 - val_loss: 0.4488 - val_accuracy: 0.9222\n",
      "Epoch 488/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3783 - accuracy: 0.9512 - val_loss: 0.4689 - val_accuracy: 0.9158\n",
      "Epoch 489/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3781 - accuracy: 0.9509 - val_loss: 0.4579 - val_accuracy: 0.9199\n",
      "Epoch 490/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3783 - accuracy: 0.9518 - val_loss: 0.4722 - val_accuracy: 0.9166\n",
      "Epoch 491/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3756 - accuracy: 0.9527 - val_loss: 0.4523 - val_accuracy: 0.9251\n",
      "Epoch 492/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3759 - accuracy: 0.9534 - val_loss: 0.4625 - val_accuracy: 0.9230\n",
      "Epoch 493/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3757 - accuracy: 0.9530 - val_loss: 0.4591 - val_accuracy: 0.9228\n",
      "Epoch 494/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3745 - accuracy: 0.9524 - val_loss: 0.4541 - val_accuracy: 0.9218\n",
      "Epoch 495/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3753 - accuracy: 0.9534 - val_loss: 0.4912 - val_accuracy: 0.9127\n",
      "Epoch 496/1000\n",
      "139/139 [==============================] - 13s 97ms/step - loss: 0.3743 - accuracy: 0.9530 - val_loss: 0.4423 - val_accuracy: 0.9243\n",
      "Epoch 497/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3729 - accuracy: 0.9535 - val_loss: 0.4516 - val_accuracy: 0.9230\n",
      "Epoch 498/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3746 - accuracy: 0.9515 - val_loss: 0.4555 - val_accuracy: 0.9239\n",
      "Epoch 499/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3748 - accuracy: 0.9542 - val_loss: 0.4622 - val_accuracy: 0.9232\n",
      "Epoch 500/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3731 - accuracy: 0.9532 - val_loss: 0.4507 - val_accuracy: 0.9214\n",
      "Epoch 501/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3683 - accuracy: 0.9558 - val_loss: 0.4510 - val_accuracy: 0.9185\n",
      "Epoch 502/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3753 - accuracy: 0.9527 - val_loss: 0.4584 - val_accuracy: 0.9203\n",
      "Epoch 503/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3714 - accuracy: 0.9544 - val_loss: 0.4589 - val_accuracy: 0.9220\n",
      "Epoch 504/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3740 - accuracy: 0.9513 - val_loss: 0.4554 - val_accuracy: 0.9185\n",
      "Epoch 505/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3717 - accuracy: 0.9528 - val_loss: 0.4584 - val_accuracy: 0.9228\n",
      "Epoch 506/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3708 - accuracy: 0.9544 - val_loss: 0.4502 - val_accuracy: 0.9239\n",
      "Epoch 507/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3691 - accuracy: 0.9548 - val_loss: 0.4463 - val_accuracy: 0.9241\n",
      "Epoch 508/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3758 - accuracy: 0.9532 - val_loss: 0.4865 - val_accuracy: 0.9139\n",
      "Epoch 509/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3726 - accuracy: 0.9534 - val_loss: 0.4676 - val_accuracy: 0.9218\n",
      "Epoch 510/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3670 - accuracy: 0.9570 - val_loss: 0.4508 - val_accuracy: 0.9239\n",
      "Epoch 511/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3715 - accuracy: 0.9530 - val_loss: 0.4593 - val_accuracy: 0.9210\n",
      "Epoch 512/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.3684 - accuracy: 0.9551 - val_loss: 0.4513 - val_accuracy: 0.9249\n",
      "Epoch 513/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3761 - accuracy: 0.9526 - val_loss: 0.4488 - val_accuracy: 0.9243\n",
      "Epoch 514/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3671 - accuracy: 0.9561 - val_loss: 0.4634 - val_accuracy: 0.9210\n",
      "Epoch 515/1000\n",
      "139/139 [==============================] - 13s 95ms/step - loss: 0.3664 - accuracy: 0.9575 - val_loss: 0.4458 - val_accuracy: 0.9272\n",
      "Epoch 516/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3698 - accuracy: 0.9535 - val_loss: 0.4584 - val_accuracy: 0.9207\n",
      "Epoch 517/1000\n",
      "139/139 [==============================] - 13s 94ms/step - loss: 0.3709 - accuracy: 0.9543 - val_loss: 0.4634 - val_accuracy: 0.9222\n",
      "Epoch 518/1000\n",
      "139/139 [==============================] - 13s 96ms/step - loss: 0.3703 - accuracy: 0.9543 - val_loss: 0.4513 - val_accuracy: 0.9234\n",
      "Epoch 519/1000\n",
      "139/139 [==============================] - 15s 107ms/step - loss: 0.3693 - accuracy: 0.9544 - val_loss: 0.4436 - val_accuracy: 0.9230\n",
      "Epoch 520/1000\n",
      "139/139 [==============================] - 14s 99ms/step - loss: 0.3663 - accuracy: 0.9558 - val_loss: 0.4462 - val_accuracy: 0.9230\n",
      "Epoch 521/1000\n",
      "139/139 [==============================] - 14s 101ms/step - loss: 0.3701 - accuracy: 0.9545 - val_loss: 0.4413 - val_accuracy: 0.9278\n",
      "Epoch 522/1000\n",
      "139/139 [==============================] - 14s 98ms/step - loss: 0.3660 - accuracy: 0.9555 - val_loss: 0.4423 - val_accuracy: 0.9232\n",
      "Epoch 523/1000\n",
      " 34/139 [======>.......................] - ETA: 8s - loss: 0.3607 - accuracy: 0.9614"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m H \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      2\u001b[0m aug\u001b[39m.\u001b[39;49mflow(trainX, trainY, batch_size\u001b[39m=\u001b[39;49mBS),\n\u001b[0;32m      3\u001b[0m validation_data\u001b[39m=\u001b[39;49m(testX, testY),\n\u001b[0;32m      4\u001b[0m steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(trainX) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m BS,epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[0;32m      5\u001b[0m class_weight\u001b[39m=\u001b[39;49mclassWeight,\n\u001b[0;32m      6\u001b[0m verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32md:\\programming\\miniconda\\envs\\DeepLearning\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "aug.flow(trainX, trainY, batch_size=BS),\n",
    "validation_data=(testX, testY),\n",
    "steps_per_epoch=len(trainX) // BS,epochs=EPOCHS,\n",
    "class_weight=classWeight,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Ukrainian_OCR_extended_Resnet_with_digits_with_correct_fonts.h5',save_format=\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('Ukrainian_OCR_extended_Resnet_with_digits_with_correct_fonts.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "205/205 [==============================] - 31s 149ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "                  0.96      0.92      0.94       145\n",
      "                  0.95      0.97      0.96       146\n",
      "                  0.93      0.97      0.95       145\n",
      "                  0.97      0.95      0.96       145\n",
      "                  0.92      0.97      0.94       146\n",
      "                  0.93      0.97      0.95       145\n",
      "                  0.94      0.96      0.95       146\n",
      "                  0.85      1.00      0.92       146\n",
      "                  0.91      0.99      0.94       145\n",
      "                  0.89      0.62      0.73       145\n",
      "                  0.93      0.97      0.95       146\n",
      "                  0.90      0.95      0.92       145\n",
      "                  0.93      0.98      0.96       146\n",
      "                  0.93      0.97      0.95       145\n",
      "                  0.95      0.96      0.95       145\n",
      "                  0.95      0.94      0.94       146\n",
      "                  0.96      0.95      0.96       145\n",
      "                  0.95      0.95      0.95       146\n",
      "                  0.69      0.95      0.80       146\n",
      "                  0.97      0.90      0.94       145\n",
      "                  0.93      0.94      0.93       145\n",
      "                  0.90      0.98      0.94       145\n",
      "                  0.90      0.98      0.94       145\n",
      "                  0.85      0.92      0.89       145\n",
      "                  0.96      0.90      0.93       145\n",
      "                  0.91      0.94      0.93       145\n",
      "                  0.92      0.99      0.95       146\n",
      "                  0.94      0.95      0.95       145\n",
      "                  0.93      0.98      0.95       146\n",
      "                  0.94      0.95      0.94       146\n",
      "                  0.92      0.99      0.95       146\n",
      "                  0.94      0.96      0.95       146\n",
      "                  0.90      0.98      0.94       146\n",
      "                  0.96      0.94      0.95       145\n",
      "                  0.97      0.94      0.96       145\n",
      "                  0.98      0.91      0.94       145\n",
      "                  0.93      0.97      0.95       145\n",
      "                  0.97      0.90      0.93       146\n",
      "                  0.93      0.93      0.93       146\n",
      "                  0.95      0.93      0.94       146\n",
      "                  1.00      0.83      0.91       145\n",
      "                  0.98      0.90      0.94       146\n",
      "                  0.95      0.93      0.94       145\n",
      "                  0.97      0.92      0.94       146\n",
      "                  0.95      0.94      0.94       145\n",
      "                  0.97      0.93      0.95       145\n",
      "                  0.96      0.92      0.94       146\n",
      "                  0.96      0.94      0.95       145\n",
      "                  0.93      0.95      0.94       146\n",
      "                  0.95      0.96      0.96       145\n",
      "                  0.94      0.94      0.94       145\n",
      "                  0.93      0.91      0.92       145\n",
      "                  0.91      0.97      0.94       146\n",
      "                  0.94      0.92      0.93       145\n",
      "                  0.98      0.90      0.94       145\n",
      "                  0.98      0.90      0.94       145\n",
      "                  0.92      0.83      0.87       145\n",
      "                  0.91      0.96      0.93       146\n",
      "                  0.94      0.91      0.92       145\n",
      "                  0.99      0.91      0.95       145\n",
      "                  0.95      0.95      0.95       146\n",
      "                  0.96      0.92      0.94       145\n",
      "                  0.95      0.93      0.94       145\n",
      "                  0.99      0.91      0.95       145\n",
      "                  0.96      0.94      0.95       145\n",
      "                  0.98      0.90      0.94       145\n",
      "           1       0.99      0.96      0.98       146\n",
      "           2       1.00      1.00      1.00       145\n",
      "           3       0.72      0.95      0.81       146\n",
      "           4       1.00      0.99      1.00       146\n",
      "           5       1.00      1.00      1.00       145\n",
      "           6       1.00      1.00      1.00       145\n",
      "           7       1.00      1.00      1.00       145\n",
      "           8       0.98      1.00      0.99       145\n",
      "           9       1.00      1.00      1.00       146\n",
      "           0       0.94      0.60      0.73       146\n",
      "                  0.98      0.96      0.97       145\n",
      "           %       1.00      0.99      1.00       145\n",
      "           @       1.00      0.97      0.98       145\n",
      "           ,       1.00      1.00      1.00       145\n",
      "           .       1.00      1.00      1.00       145\n",
      "           ?       1.00      1.00      1.00       145\n",
      "           :       1.00      1.00      1.00       145\n",
      "           ;       1.00      1.00      1.00       145\n",
      "           \"       0.99      0.99      0.99       145\n",
      "           !       1.00      0.99      1.00       145\n",
      "           (       1.00      0.99      0.99       145\n",
      "           )       1.00      0.99      1.00       146\n",
      "           -       0.99      1.00      1.00       146\n",
      "           '       0.95      0.99      0.97       145\n",
      "\n",
      "    accuracy                           0.95     13082\n",
      "   macro avg       0.95      0.95      0.95     13082\n",
      "weighted avg       0.95      0.95      0.95     13082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# define the list of label names\n",
    "\n",
    "labelNames = \"1234567890%@,.?:;\\\"!()-'\"\n",
    "labelNames = [l for l in labelNames]\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "from imutils import build_montages\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "images = []\n",
    "# randomly select a few testing characters\n",
    "\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(66,)):\n",
    "    probs = model.predict(testX[np.newaxis, i])\n",
    "    prediction = probs.argmax(axis=1)\n",
    "    label = labelNames[prediction[0]]\n",
    "    output += label\n",
    "    image = (testX[i] * 255).astype(\"uint8\")\n",
    "    color = (0, 255, 0)\n",
    "    color = (0, 0, 255)\n",
    "    image = cv2.merge([image] * 3)\n",
    "    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "    img_pil = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", size=34)\n",
    "    draw.text((5, 20), label, font=font, fill=color)\n",
    "    image = np.array(img_pil)\n",
    "    images.append(image)\n",
    "  \n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "cv2.imshow('q',montage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
